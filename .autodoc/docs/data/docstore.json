[["0",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/__init__.py)\n\nThe code provided is part of the `turbo-chat` project and serves as an entry point for importing various modules and classes. It imports all the necessary components from different submodules, such as bots, cache, chat, config, errors, memory, structs, turbo, types, and utils. By importing these components, it makes them available for use in other parts of the project.\n\nThe `__all__` list defines the public interface of this module, specifying which classes, functions, and variables should be accessible when importing the module. This list includes:\n\n- `System`, `User`, `Assistant`, `ExampleUser`, and `ExampleAssistant` classes, which are likely used to represent different types of users and assistants in the chat system.\n- `Generate` and `GetInput` functions, which might be used for generating responses and getting user input, respectively.\n- Error classes like `InvalidValueYieldedError` and `GeneratorAlreadyExhaustedError`, which handle specific error scenarios.\n- `TurboGenWrapper` class, which could be a wrapper for generator functions.\n- `MessageRole`, `Message`, and `BaseMessageCollection` classes, which are probably used for handling chat messages and their roles (e.g., sender, receiver).\n- Memory-related classes like `BaseMemory`, `LocalMemory`, `LocalSummarizeMemory`, and `LocalTruncatedMemory`, which might be used for storing and managing chat history or other data.\n- `Example` class, which could be used for creating example chat scenarios.\n- Cache-related classes like `BaseCache`, `SimpleCache`, and `Scratchpad`, which might be used for caching data and managing temporary storage.\n- `Result` and `Tool` classes, which could be used for processing and managing the results of chat interactions.\n- Functions like `turbo`, `summarize_bot`, `subqueries_bot`, `tool_bot`, `self_ask_bot`, and `qa_bot`, which might be different types of bots or tools used in the chat system.\n- `available_models` variable, which could be a list of available models for use in the chat system.\n\nBy providing a single entry point for importing these components, the code simplifies the process of using them in other parts of the project. For example, to use the `User` class, one would simply write:\n\n```python\nfrom turbo_chat import User\n```\n\nThis makes it easier to maintain and understand the project structure.\n## Questions: \n 1. **What is the purpose of the `flake8: noqa` comment at the beginning of the code?**\n\n   The `flake8: noqa` comment is used to tell the Flake8 linter to ignore this file when checking for code style violations. This is usually done when the developer believes that the code style rules should not be applied to this specific file.\n\n2. **What is the purpose of the `__all__` variable in this code?**\n\n   The `__all__` variable is a list that defines the public interface of the module. It specifies which names should be imported when a client imports the module using a wildcard import (e.g., `from module import *`). This helps to keep the namespace clean and avoid importing unnecessary or internal names.\n\n3. **Are there any dependencies or external libraries used in this code?**\n\n   It is not possible to determine the dependencies or external libraries used in this code from the given snippet. The code only imports various components from submodules within the `turbo-chat` package, but it does not show the contents of those submodules. To determine the dependencies, one would need to examine the imported submodules.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/__init__.md"}}],["1",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/__init__.py)\n\nThis code is part of the `turbo-chat` project and serves as a central module that imports and exports various functionalities related to chatbots. The purpose of this module is to provide a single point of access to the different chatbot functionalities, making it easier for other parts of the project to use them.\n\nThe code starts by importing all the necessary modules from their respective subdirectories:\n\n- `qa`: This module contains the implementation of a question-answering chatbot, which can be used to answer questions based on a given context.\n- `self_ask`: This module provides a chatbot that can ask itself questions and generate answers, simulating a self-reflective conversation.\n- `subqueries`: This module contains a chatbot that can handle subqueries, or nested questions, within a larger conversation.\n- `summarize`: This module provides a chatbot that can summarize long pieces of text, making it easier for users to understand the main points.\n- `tool`: This module contains various utility functions and tools that can be used by the other chatbot modules.\n\nAfter importing the modules, the code defines a list called `__all__`, which contains the names of the chatbot functionalities that should be exported by this module. This list includes:\n\n- `qa_bot`: The question-answering chatbot from the `qa` module.\n- `self_ask_bot`: The self-asking chatbot from the `self_ask` module.\n- `subqueries_bot`: The subqueries chatbot from the `subqueries` module.\n- `summarize_bot`: The summarizing chatbot from the `summarize` module.\n- `tool_bot`: The utility functions and tools from the `tool` module.\n\nBy including these chatbot functionalities in the `__all__` list, the module ensures that they can be easily imported and used by other parts of the `turbo-chat` project. For example, to use the question-answering chatbot, one could simply write:\n\n```python\nfrom turbo_chat import qa_bot\n\nanswer = qa_bot.ask(\"What is the capital of France?\")\nprint(answer)\n```\n\nThis central module helps keep the project organized and makes it easier for developers to access and use the various chatbot functionalities provided by the `turbo-chat` project.\n## Questions: \n 1. **What is the purpose of `# flake8: noqa` at the beginning of the code?**\n\n   The `# flake8: noqa` comment is used to tell the Flake8 linter to ignore this file when checking for style and syntax issues, as the developer might have intentionally used wildcard imports or other non-standard practices in this specific file.\n\n2. **What are the different modules being imported and what functionality do they provide?**\n\n   The code imports five modules: `qa`, `self_ask`, `subqueries`, `summarize`, and `tool`. Each module likely contains a specific bot or functionality related to the turbo-chat project, such as handling question-answering, self-asking questions, managing subqueries, summarizing content, and providing additional tools.\n\n3. **What is the purpose of the `__all__` variable in this code?**\n\n   The `__all__` variable is used to define the public interface of the module. It is a list of strings that specifies which names should be imported when a client imports the module using a wildcard import (e.g., `from turbo_chat import *`). In this case, it includes the five bot names: `qa_bot`, `self_ask_bot`, `subqueries_bot`, `summarize_bot`, and `tool_bot`.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/__init__.md"}}],["2",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/qa.py)\n\nThe `turbo-chat` project contains a file that defines a `qa_bot` function, which is designed to generate answers to questions based on a given context. This function is part of a larger project that likely involves a chatbot or question-answering system.\n\nThe `qa_bot` function is defined as an asynchronous function, which means it can be used in an asynchronous context, allowing for better performance and concurrency. It takes two arguments: `question` and `context`. The `question` is a string representing the question to be answered, and the `context` is a string containing the information needed to answer the question.\n\nThe function uses a template string called `TEMPLATE` to format the input data. The template is structured with a \"START CONTEXT\" and \"END CONTEXT\" section, followed by the question. The context and question are inserted into the template using the `variables` dictionary.\n\nThe `qa_bot` function is decorated with the `@turbo` decorator, which likely comes from the larger project and is used to control the generation process. The `temperature` parameter is set to 0.1, which typically controls the randomness of the generated output. A lower temperature value results in more focused and deterministic output, while a higher value produces more diverse and creative output.\n\nInside the function, the `User` object is created with the formatted template and variables. This object is then yielded, which means it will be returned as part of an asynchronous generator. After that, the `Generate` object is yielded, which is likely used by the larger project to trigger the actual generation of the answer based on the provided context and question.\n\nHere's an example of how the `qa_bot` function might be used in the larger project:\n\n```python\nasync def main():\n    question = \"What is the capital of France?\"\n    context = \"France is a country in Europe. Its capital is Paris.\"\n    async for response in qa_bot(question, context):\n        print(response)\n\nawait main()\n```\n\nIn summary, the code defines a `qa_bot` function that takes a question and context as input and generates an answer based on them. The function is asynchronous and uses a template to format the input data before generating the response.\n## Questions: \n 1. **What is the purpose of the `turbo` decorator?**\n\n   The `turbo` decorator might be used to modify the behavior of the `qa_bot` function, possibly related to the temperature parameter. More information about the `turbo` decorator and its functionality would be needed to understand its exact purpose.\n\n2. **How is the `TEMPLATE` string used in the `qa_bot` function?**\n\n   The `TEMPLATE` string is used as a template for generating the input for the User object. It formats the given question and context within the template, which is then passed to the User object as the `template` parameter.\n\n3. **What is the purpose of the `Generate` object in the `qa_bot` function?**\n\n   The `Generate` object is yielded after the User object, which might indicate that it is used to trigger the generation of an answer based on the provided question and context. More information about the `Generate` object and its functionality would be needed to understand its exact purpose.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/qa.md"}}],["3",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/self_ask.py)\n\nThe `self_ask_bot` function in this code is designed to answer a given question step by step using a provided `qa_bot`. It does this by first generating sub-queries related to the main question and then answering each sub-query before finally answering the main question. This approach allows the AI to break down complex questions into smaller, more manageable parts, which can lead to more accurate and comprehensive answers.\n\nThe function starts by calling the `subqueries_bot` function, which generates a list of sub-queries related to the main question. The `subquery_instructions` parameter provides context for generating these sub-queries, with a default value of \"User is asking questions to an AI assistant.\"\n\nNext, the function iterates through the list of sub-queries and the main question, answering each one using the provided `qa_bot`. The `make_qa_context` function is used to prepare the context for each question, which includes the original context and any previous question-answer pairs. This allows the AI to reference previous answers when generating new ones, helping to maintain consistency and coherence.\n\nAfter answering each question, the function appends the question and answer to the `previous_qa` list. Once all questions have been answered, the function yields the final answer as a `Result` object.\n\nHere's an example of how the `self_ask_bot` function might be used in the larger project:\n\n```python\n# Define a question and context\nquestion = \"How does photosynthesis work?\"\ncontext = \"Photosynthesis is a process used by plants to convert sunlight into energy.\"\n\n# Call the self_ask_bot function with a custom qa_bot\nanswer = await self_ask_bot(question, context, custom_qa_bot).run()\n\n# Print the final answer\nprint(answer.content)\n```\n\nIn this example, the `self_ask_bot` function would generate sub-queries related to photosynthesis, answer each one using the `custom_qa_bot`, and then provide a final answer to the main question.\n## Questions: \n 1. **What is the purpose of the `make_qa_context` function?**\n\n   The `make_qa_context` function is a utility function that prepares the QA context by adding previous question and answer pairs as FAQs to the given context.\n\n2. **How does the `self_ask_bot` function work?**\n\n   The `self_ask_bot` function takes a question, context, and a QA bot as inputs, generates sub-queries using the `subqueries_bot`, answers the sub-questions, and then yields the final answer as the result.\n\n3. **What is the role of the `subquery_instructions` parameter in the `self_ask_bot` function?**\n\n   The `subquery_instructions` parameter is used to provide additional context or instructions to the `subqueries_bot` when generating sub-queries for the given question. By default, it is set to \"User is asking questions to an AI assistant.\"","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/self_ask.md"}}],["4",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/subqueries/__init__.py)\n\nThe code provided is a part of the `turbo-chat` project and serves as an entry point for importing the `subqueries_bot` functionality. This file is responsible for exposing the `subqueries_bot` object to other modules within the project, allowing them to utilize its features.\n\nThe first line, `# flake8: noqa`, is a directive for the Flake8 linter, which is a tool used to check Python code for adherence to coding standards and conventions. The `noqa` directive tells Flake8 to ignore any warnings or errors in this specific file, as they may not be relevant or necessary to address.\n\nNext, the code imports all objects from the `.bot` module using the wildcard import statement `from .bot import *`. This means that all objects defined in the `.bot` module will be available in the current module's namespace. The use of the dot before `bot` indicates that it is a relative import, meaning the `bot` module is located in the same package as the current file.\n\nAfter that, the `__all__` variable is defined as a list containing the string `\"subqueries_bot\"`. The `__all__` variable is a special variable in Python that defines the public interface of a module. When another module imports this one using a wildcard import (e.g., `from turbo_chat import *`), only the objects listed in the `__all__` variable will be imported. In this case, only the `subqueries_bot` object will be imported, ensuring that other objects from the `.bot` module remain private and are not unintentionally exposed.\n\nIn summary, this code serves as an entry point for the `turbo-chat` project to import and utilize the `subqueries_bot` functionality. It ensures that only the necessary objects are exposed to other modules, maintaining a clean and organized project structure.\n## Questions: \n 1. **Question:** What is the purpose of the `flake8: noqa` comment at the beginning of the code?\n   **Answer:** The `flake8: noqa` comment is used to tell the Flake8 linter to ignore this file when checking for style and syntax issues, as the developer might have intentionally written the code in a way that doesn't adhere to the standard style guide.\n\n2. **Question:** What does the `from .bot import *` statement do?\n   **Answer:** The `from .bot import *` statement imports all the objects (functions, classes, variables, etc.) defined in the `bot` module located in the same package as this file, making them available for use in this module.\n\n3. **Question:** What is the purpose of the `__all__` variable in this code?\n   **Answer:** The `__all__` variable is used to define the public interface of this module, specifying which objects should be imported when a client imports this module using a wildcard import (e.g., `from turbo_chat import *`). In this case, only the `subqueries_bot` object will be imported.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/subqueries/__init__.md"}}],["5",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/subqueries/bot.py)\n\nThe `subqueries_bot` function in this code is designed to decompose a given request into a series of subqueries that can be used to query a knowledgebase. This is particularly useful in situations where a single question may require multiple pieces of information to be retrieved from the knowledgebase in order to provide a comprehensive answer.\n\nThe function takes several input parameters, including the main request, context, an example (defaulting to `default_subqueries_example`), maximum number of subqueries allowed (`max_queries`), request type, and the action to perform (e.g., \"answer\").\n\nFirst, the function yields a `User` object with a template and variables. The template, `SUBQUERIES_TEMPLATE`, is used to format the input parameters into a structured format that the model can understand. The variables include the request, context, example, maximum number of subqueries, request type, and action to perform.\n\nNext, the function generates an output using the `Generate` method with the `forward` parameter set to `False`. This output is then parsed using the `scratchpad.parse` method, which extracts the subqueries from the generated content.\n\nFinally, the function filters out any `None` values from the parsed subqueries and returns the list of subqueries as the result.\n\nHere's an example of how the `subqueries_bot` function might be used:\n\n```python\nrequest = \"What is the least expensive cereal that is healthy and has a low calorie content but is also tasty?\"\ncontext = \"User is a customer at a grocery store and is asking the question to the store manager.\"\n\nsubqueries = await subqueries_bot(request, context)\nprint(subqueries)\n```\n\nThis would output a list of subqueries like:\n\n```\n[\n    \"What are some tasty cereal that are healthy?\",\n    \"What are the prices of the above cereals?\",\n    \"What is the least expensive cereal of the above?\"\n]\n```\n\nThese subqueries can then be used to query the knowledgebase and gather the necessary information to answer the original request.\n## Questions: \n 1. **Question:** What is the purpose of the `default_subqueries_example` variable?\n   **Answer:** The `default_subqueries_example` variable provides a default example string that demonstrates how to use the `subqueries_bot` function. It is used as the default value for the `example` parameter in the function.\n\n2. **Question:** How does the `subqueries_bot` function handle the input parameters and generate the subqueries?\n   **Answer:** The `subqueries_bot` function takes the input parameters (request, context, example, max_queries, request_type, and solve_act) and uses them to create a `User` object with the `SUBQUERIES_TEMPLATE`. It then generates output using the `Generate` function and parses the output using the `scratchpad.parse` function to obtain the subqueries.\n\n3. **Question:** What is the purpose of the `__all__` variable in the code?\n   **Answer:** The `__all__` variable is used to define the public interface of the module. It specifies which names should be imported when a client imports the module using a wildcard import (e.g., `from module import *`). In this case, it indicates that only the `subqueries_bot` function should be imported.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/subqueries/bot.md"}}],["6",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/subqueries/scratchpad.py)\n\nThis code defines a data structure and a template for handling parsed queries in the `turbo-chat` project. The main purpose of this code is to provide a consistent way to store and manage multiple parsed queries, which can be used in various parts of the project.\n\nThe code starts by importing `Optional` and `TypedDict` from the `typing` module, and `Scratchpad` from the `structs` module in the project. The `__all__` list is defined to specify the public interface of this module, which includes `ParsedQueries` and `scratchpad`.\n\n`ParsedQueries` is a custom dictionary class that inherits from `TypedDict`. It defines a structure for storing up to 10 parsed queries, with each query being an optional string. This means that each query can either have a string value or be `None`. The keys for the queries are named `query1` through `query10`.\n\n```python\nclass ParsedQueries(TypedDict):\n    query1: Optional[str]\n    query2: Optional[str]\n    query3: Optional[str]\n    query4: Optional[str]\n    query5: Optional[str]\n    query6: Optional[str]\n    query7: Optional[str]\n    query8: Optional[str]\n    query9: Optional[str]\n    query10: Optional[str]\n```\n\nThe `scratchpad` variable is an instance of the `Scratchpad` class, which is parameterized with the `ParsedQueries` type. This means that the `scratchpad` object will store data in the format defined by the `ParsedQueries` class. The `scratchpad` object is initialized with a multiline string that serves as a template for displaying the parsed queries. The string contains placeholders for each query, enclosed in curly braces, which will be replaced with the actual query values when the `scratchpad` object is used.\n\n```python\nscratchpad: Scratchpad[ParsedQueries] = Scratchpad[ParsedQueries](\n    \"\"\"\n1. {query1}\n2. {query2}\n3. {query3}\n4. {query4}\n5. {query5}\n6. {query6}\n7. {query7}\n8. {query8}\n9. {query9}\n10. {query10}\n\"\"\".strip()\n)\n```\n\nIn the larger project, this code can be used to store and manage parsed queries from user input or other sources. The `ParsedQueries` structure ensures that the queries are stored in a consistent format, while the `scratchpad` object provides a convenient way to display and manipulate the queries.\n## Questions: \n 1. **Question:** What is the purpose of the `ParsedQueries` class and why are all the attributes `Optional`?\n   **Answer:** The `ParsedQueries` class is a TypedDict that represents a dictionary with specific keys and types for its values. All the attributes are `Optional` because they may or may not have a value, allowing for flexibility in the number of queries being used.\n\n2. **Question:** How is the `scratchpad` variable being used and what is its purpose?\n   **Answer:** The `scratchpad` variable is an instance of the `Scratchpad` class, which is parameterized with the `ParsedQueries` TypedDict. It is used to store and manage the parsed queries in a structured format.\n\n3. **Question:** What is the purpose of the `__all__` variable in this code?\n   **Answer:** The `__all__` variable is used to define the public interface of this module. It specifies which names should be imported when a client imports this module using a wildcard import (e.g., `from module import *`). In this case, `ParsedQueries` and `scratchpad` are the names that will be imported.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/subqueries/scratchpad.md"}}],["7",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/tree/master/.autodoc/docs/json/turbo_chat/bots/subqueries)\n\nThe `subqueries_bot` functionality in the `turbo-chat` project is designed to decompose a given request into a series of subqueries that can be used to query a knowledgebase. This is particularly useful in situations where a single question may require multiple pieces of information to be retrieved from the knowledgebase in order to provide a comprehensive answer.\n\nThe main function, `subqueries_bot`, takes several input parameters, including the main request, context, an example (defaulting to `default_subqueries_example`), maximum number of subqueries allowed (`max_queries`), request type, and the action to perform (e.g., \"answer\"). It yields a `User` object with a template and variables, generates an output using the `Generate` method, and parses the output to extract the subqueries.\n\nFor example, consider the following usage:\n\n```python\nrequest = \"What is the least expensive cereal that is healthy and has a low calorie content but is also tasty?\"\ncontext = \"User is a customer at a grocery store and is asking the question to the store manager.\"\n\nsubqueries = await subqueries_bot(request, context)\nprint(subqueries)\n```\n\nThis would output a list of subqueries like:\n\n```\n[\n    \"What are some tasty cereal that are healthy?\",\n    \"What are the prices of the above cereals?\",\n    \"What is the least expensive cereal of the above?\"\n]\n```\n\nThese subqueries can then be used to query the knowledgebase and gather the necessary information to answer the original request.\n\nThe `scratchpad.py` file defines a data structure and a template for handling parsed queries. The `ParsedQueries` class defines a structure for storing up to 10 parsed queries, with each query being an optional string. The `scratchpad` object is an instance of the `Scratchpad` class, which is parameterized with the `ParsedQueries` type and initialized with a multiline string that serves as a template for displaying the parsed queries.\n\nThe `template.py` file defines a template for generating instructions to create a plan for collecting information to answer a complex question or solve a problem. The template, `SUBQUERIES_TEMPLATE`, is a multi-line string that uses Jinja2 syntax for variable substitution and control structures. It takes several optional parameters, such as `request_type`, `solve_act`, and `max_queries`, which have default values if not provided.\n\nHere's a sample usage of the template:\n\n```python\nfrom jinja2 import Template\n\ntemplate = Template(SUBQUERIES_TEMPLATE)\noutput = template.render(request_type=\"question\", solve_act=\"answer\", max_queries=5, context=\"A complex math problem\", request=\"Solve the equation x^2 + 2x - 3 = 0\")\nprint(output)\n```\n\nThis would generate instructions for creating a plan to collect information for solving the given math problem, with a maximum of 5 queries to the knowledgebase.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/subqueries/summary.md"}}],["8",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/subqueries/template.py)\n\nThis code defines a template for generating instructions to create a plan for collecting information to answer a complex question or solve a problem. The template is designed to be used in the larger turbo-chat project, where users interact with a knowledgebase that can provide answers to plain English queries.\n\nThe template, `SUBQUERIES_TEMPLATE`, is a multi-line string that uses Jinja2 syntax for variable substitution and control structures. It takes several optional parameters, such as `request_type`, `solve_act`, and `max_queries`, which have default values if not provided. The template guides the user through a step-by-step process to break down a complex request into individual, standalone queries that can be executed by the knowledgebase.\n\nThe instructions in the template are divided into two parts: \"Thoughts\" and \"Queries to execute\". In the \"Thoughts\" section, users are encouraged to think about the topic and required information for solving the request. In the \"Queries to execute\" section, users are instructed to write down a numbered list of concise, standalone queries, with a maximum limit of `max_queries` (default is 6).\n\nThe template also provides an example format for users to follow when writing down their thoughts and queries. The example is enclosed within \"START EXAMPLE\" and \"END EXAMPLE\" markers.\n\nHere's a sample usage of the template:\n\n```python\nfrom jinja2 import Template\n\ntemplate = Template(SUBQUERIES_TEMPLATE)\noutput = template.render(request_type=\"question\", solve_act=\"answer\", max_queries=5, context=\"A complex math problem\", request=\"Solve the equation x^2 + 2x - 3 = 0\")\nprint(output)\n```\n\nThis would generate instructions for creating a plan to collect information for solving the given math problem, with a maximum of 5 queries to the knowledgebase.\n## Questions: \n 1. **Question:** What is the purpose of the `SUBQUERIES_TEMPLATE` variable in this code?\n\n   **Answer:** The `SUBQUERIES_TEMPLATE` variable holds a template string that is used to generate instructions for creating a plan to collect information for answering a complex question. It includes placeholders for various parameters and a specific format for writing down thoughts and queries.\n\n2. **Question:** What are the default values for `_request_type`, `_solve_act`, and `_max_queries` in the template?\n\n   **Answer:** The default values for `_request_type`, `_solve_act`, and `_max_queries` are \"question\", \"answer\", and 6, respectively.\n\n3. **Question:** How does the template handle inflection for the `_solve_act` variable?\n\n   **Answer:** The template uses the `inflect(\"VBG\")` filter to convert the `_solve_act` variable into its gerund form (e.g., \"answering\" for the default value \"answer\").","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/subqueries/template.md"}}],["9",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/summarize.py)\n\nThe `turbo-chat` project contains a file that defines a function called `summarize_bot`. This function is designed to generate a summary of a given text using OpenAI's GPT-3.5 Turbo model. The purpose of this code is to provide a high-level interface for summarizing text within the larger project.\n\nThe `summarize_bot` function takes two arguments: `text` and `text_type`. The `text` argument is the input text that needs to be summarized, while the `text_type` argument is an optional parameter that describes the type of text being summarized (e.g., \"article\", \"email\", etc.). By default, `text_type` is set to \"text\".\n\nThe function uses a template called `SUMMARIZE_TEMPLATE` to format the input for the GPT-3.5 Turbo model. This template is a string that includes placeholders for the `text_type` and `text` variables. When the function is called, these placeholders are replaced with the actual values provided by the user.\n\nThe `summarize_bot` function is defined as an asynchronous function using the `async def` syntax. This means that it can be used with Python's `asyncio` library for concurrent execution, which is useful when working with the GPT-3.5 Turbo model, as it allows for efficient handling of multiple requests.\n\nThe function is decorated with the `@turbo` decorator, which indicates that it should use the GPT-3.5 Turbo model for generating the summary. The `temperature` parameter is set to 0.2, which controls the randomness of the generated text. Lower values make the output more focused and deterministic, while higher values make it more random.\n\nHere's an example of how the `summarize_bot` function might be used in the larger project:\n\n```python\nimport asyncio\nfrom turbo_chat import summarize_bot\n\nasync def main():\n    text = \"This is a sample text that needs to be summarized.\"\n    summary = await summarize_bot(text, text_type=\"text\")\n    print(summary)\n\nasyncio.run(main())\n```\n\nIn this example, the `main` function is defined as asynchronous and calls the `summarize_bot` function with a sample text. The summary is then printed to the console.\n## Questions: \n 1. **Question:** What is the purpose of the `summarize_bot` function?\n   **Answer:** The `summarize_bot` function is an asynchronous function that takes a given text and its type as input, and generates a summary using the GPT-3.5-turbo model with a temperature of 0.2.\n\n2. **Question:** How is the `SUMMARIZE_TEMPLATE` used in the `summarize_bot` function?\n   **Answer:** The `SUMMARIZE_TEMPLATE` is a string template that formats the input text and text_type into a prompt for the GPT-3.5-turbo model. It is used in the `summarize_bot` function to create a `User` object with the formatted prompt.\n\n3. **Question:** What is the purpose of the `__all__` variable in the code?\n   **Answer:** The `__all__` variable is a list that defines the public interface of the module. It specifies which names should be imported when a client imports the module using a wildcard import (e.g., `from module import *`). In this case, only the `summarize_bot` function is included in the public interface.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/summarize.md"}}],["10",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/tree/master/.autodoc/docs/json/turbo_chat/bots)\n\nThe `turbo-chat` project provides various chatbot functionalities, such as question-answering, self-asking, subqueries handling, and text summarization. These functionalities are organized in separate modules and can be easily imported and used by other parts of the project.\n\nFor example, the `qa_bot` function in the `qa` module generates answers to questions based on a given context. It is an asynchronous function that uses a template string to format the input data and the `@turbo` decorator to control the generation process. Here's a sample usage:\n\n```python\nfrom turbo_chat import qa_bot\n\nanswer = qa_bot.ask(\"What is the capital of France?\")\nprint(answer)\n```\n\nThe `self_ask_bot` function in the `self_ask` module answers a given question step by step using a provided `qa_bot`. It generates sub-queries related to the main question and answers each sub-query before finally answering the main question. This approach allows the AI to break down complex questions into smaller parts. Here's an example:\n\n```python\nquestion = \"How does photosynthesis work?\"\ncontext = \"Photosynthesis is a process used by plants to convert sunlight into energy.\"\nanswer = await self_ask_bot(question, context, custom_qa_bot).run()\nprint(answer.content)\n```\n\nThe `summarize_bot` function in the `summarize` module generates a summary of a given text using OpenAI's GPT-3.5 Turbo model. It is an asynchronous function that uses a template to format the input data. Here's a sample usage:\n\n```python\nimport asyncio\nfrom turbo_chat import summarize_bot\n\nasync def main():\n    text = \"This is a sample text that needs to be summarized.\"\n    summary = await summarize_bot(text, text_type=\"text\")\n    print(summary)\n\nasyncio.run(main())\n```\n\nThe `subqueries_bot` functionality in the `subqueries` module decomposes a given request into a series of subqueries that can be used to query a knowledgebase. This is useful when a single question requires multiple pieces of information to provide a comprehensive answer. Here's an example:\n\n```python\nrequest = \"What is the least expensive cereal that is healthy and has a low calorie content but is also tasty?\"\ncontext = \"User is a customer at a grocery store and is asking the question to the store manager.\"\n\nsubqueries = await subqueries_bot(request, context)\nprint(subqueries)\n```\n\nThe `tool_bot` functionality in the `tool` module handles user queries and provides responses using a set of tools. It is designed to work with the OpenAI GPT-3.5-turbo model. Here's an example of how the `tool_bot` function might be used:\n\n```python\ntools = [GetInformation, Calculate, Translate]\nprologue = \"Welcome to Turbo Chat!\"\nuser_type = \"customer\"\ninstruction = \"Ask me anything, and I'll try to help you using my available tools.\"\n\nawait tool_bot(tools, prologue, user_type, instruction)\n```\n\nThese chatbot functionalities help keep the project organized and make it easier for developers to access and use the various chatbot functionalities provided by the `turbo-chat` project.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/summary.md"}}],["11",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/tool/__init__.py)\n\nThe code provided is part of the `turbo-chat` project and serves as an entry point for importing the `tool_bot` functionality. The main purpose of this code is to make it easy for other modules within the project to access and use the `tool_bot` class and its associated methods.\n\nAt the beginning of the code, there is a comment `# flake8: noqa`. This is a directive for the Flake8 linter, a popular Python code analysis tool, to ignore this file when checking for coding style violations. This is likely because the file is simple and doesn't require strict adherence to coding standards.\n\nNext, the code imports everything from the `.bot` module using a relative import statement: `from .bot import *`. This means that all classes, functions, and variables defined in the `.bot` module will be available in the current module's namespace. The use of the wildcard `*` in the import statement is generally discouraged in Python, as it can lead to name clashes and make it difficult to determine which names are actually being imported. However, in this case, it is acceptable because the purpose of this file is to provide a single entry point for importing the `tool_bot` functionality.\n\nFinally, the code defines a list called `__all__` containing the string `\"tool_bot\"`. The `__all__` variable is a special variable in Python that defines the public interface of a module. When another module imports this one using a wildcard import (e.g., `from turbo_chat import *`), only the names listed in `__all__` will be imported. In this case, the `tool_bot` class will be the only name imported, ensuring that other modules can easily access and use it without importing any unnecessary names.\n\nIn summary, this code serves as an entry point for the `tool_bot` functionality in the `turbo-chat` project, making it easy for other modules to access and use the `tool_bot` class and its associated methods. The use of the `__all__` variable ensures that only the necessary names are imported when using wildcard imports.\n## Questions: \n 1. **Question:** What is the purpose of the `flake8: noqa` comment at the beginning of the file?\n   **Answer:** The `flake8: noqa` comment is used to tell the Flake8 linter to ignore this file when checking for code style violations, allowing the developer to bypass any style-related warnings or errors for this specific file.\n\n2. **Question:** What does the `from .bot import *` statement do?\n   **Answer:** The `from .bot import *` statement imports all the objects (functions, classes, variables, etc.) from the `bot` module located in the same package as this file, making them available for use in this module.\n\n3. **Question:** What is the purpose of the `__all__` variable in this file?\n   **Answer:** The `__all__` variable is used to define the public interface of this module, specifying which objects should be imported when a client imports this module using a wildcard import (e.g., `from turbo_chat import *`). In this case, only the `tool_bot` object will be imported.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/tool/__init__.md"}}],["12",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/tool/bot.py)\n\nThe `tool_bot` function in this code is an asynchronous function that acts as a chatbot for handling user queries and providing responses using a set of tools. It is designed to work with the OpenAI GPT-3.5-turbo model and takes a list of tools, an optional prologue, user type, instruction, example, and a maximum number of iterations as input parameters.\n\nThe function starts by yielding instructions to the user using the `TOOLBOT_TEMPLATE`. It then provides an example of how to use the chatbot using the `EXAMPLE_TEMPLATE` and the `default_tool_example` string.\n\nThe main part of the function is a loop that continuously listens for user input and processes it. It first gets the user input and then starts a tool agent to parse the input and determine if a tool is required to generate a response. If no tool is required, the loop continues to the next iteration.\n\nIf a tool is required, the function checks if the selected tool is valid by comparing it to the list of available tools. If the tool is not valid, it informs the user and continues to the next iteration. If the tool is valid, it runs the tool with the provided input and yields the result.\n\nThe loop continues until a final response is generated or the maximum number of iterations is reached. The final response is then sent to the user.\n\nHere's an example of how the `tool_bot` function might be used in the larger project:\n\n```python\ntools = [GetInformation, Calculate, Translate]\nprologue = \"Welcome to Turbo Chat!\"\nuser_type = \"customer\"\ninstruction = \"Ask me anything, and I'll try to help you using my available tools.\"\n\nawait tool_bot(tools, prologue, user_type, instruction)\n```\n\nIn this example, the chatbot is initialized with a set of tools (GetInformation, Calculate, Translate), a welcome message, a user type, and an instruction. The chatbot then listens for user input and processes it using the provided tools to generate responses.\n## Questions: \n 1. **Question:** What is the purpose of the `tool_bot` function and what are its inputs?\n   **Answer:** The `tool_bot` function is an asynchronous function that takes a list of tools, an optional prologue, user_type, instruction, example, and max_iterations as inputs. It is designed to interact with the user, process their input, and use the provided tools to generate appropriate responses.\n\n2. **Question:** How does the `tool_bot` function handle invalid tools?\n   **Answer:** If the selected tool is not in the list of valid tool names, the `tool_bot` function yields a message to the user indicating that the selected tool is not valid and provides a list of valid tools to choose from.\n\n3. **Question:** How does the `tool_bot` function determine when to stop iterating and provide a final response?\n   **Answer:** The function keeps iterating until either the \"final_response\" key is found in the parsed_tools dictionary or the number of iterations left reaches 0. If a final response is found, it is sent to the user; otherwise, a default message is sent indicating that the function is not sure how to answer the question.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/tool/bot.md"}}],["13",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/tool/scratchpad.py)\n\nThis code defines a data structure and a template for handling tools in the Turbo-Chat project. The main purpose of this code is to manage the parsed information related to tools, such as their names, inputs, and responses, in a structured and organized manner.\n\nThe code starts by importing `Optional` and `TypedDict` from the `typing` module, and `Scratchpad` from the `structs` module in the project. It then defines the `__all__` variable, which is a list containing the names of the public objects that should be imported when the module is imported using a wildcard (e.g., `from module import *`).\n\nThe `ParsedTools` class is a `TypedDict`, which is a dictionary with a fixed set of keys and their corresponding value types. This class has four keys: `should_use_tool`, `tool_name`, `tool_input`, and `final_response`. Each key has an `Optional` value type, meaning that the value can be either of the specified type or `None`. This allows for flexibility in handling cases where some information might not be available.\n\nThe `scratchpad` variable is an instance of the `Scratchpad` class, which is a generic class for handling templates. In this case, the `Scratchpad` class is instantiated with the `ParsedTools` type, meaning that it will handle templates related to the `ParsedTools` data structure. The template string provided to the `Scratchpad` instance is a multi-line string containing placeholders for the `tool_name`, `tool_input`, and `final_response` fields. The `.strip()` method is called on the string to remove any leading or trailing whitespace.\n\nIn the larger Turbo-Chat project, this code can be used to manage and format the information related to tools. For example, when a tool is used in the chat, the parsed information can be stored in a `ParsedTools` instance, and the `scratchpad` can be used to generate a formatted output for displaying the tool's information in the chat.\n\nHere's an example of how this code might be used:\n\n```python\nparsed_tools = ParsedTools(\n    should_use_tool=True,\n    tool_name=\"Example Tool\",\n    tool_input=\"Sample Input\",\n    final_response=\"Sample Output\"\n)\n\nformatted_output = scratchpad.format(parsed_tools)\nprint(formatted_output)\n```\n\nThis would output:\n\n```\nTool: Example Tool\nTool Input: Sample Input\nResponse: Sample Output\n```\n## Questions: \n 1. **Question:** What is the purpose of the `ParsedTools` class and what are its attributes?\n   **Answer:** The `ParsedTools` class is a TypedDict that represents the structure of parsed tools data. It has four attributes: `should_use_tool`, `tool_name`, `tool_input`, and `final_response`, all of which are optional.\n\n2. **Question:** How is the `scratchpad` variable being used and what is its purpose?\n   **Answer:** The `scratchpad` variable is an instance of the `Scratchpad` class with the type `ParsedTools`. It is used to store and manage the parsed tools data in a structured format, using a template string to define the layout.\n\n3. **Question:** What is the purpose of the `__all__` variable in this code?\n   **Answer:** The `__all__` variable is a list that defines the public interface of this module. It specifies which names should be imported when a client imports this module using a wildcard import (e.g., `from module import *`). In this case, `ParsedTools` and `scratchpad` are the names that will be imported.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/tool/scratchpad.md"}}],["14",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/tree/master/.autodoc/docs/json/turbo_chat/bots/tool)\n\nThe `tool_bot` folder contains code for a chatbot that handles user queries and provides responses using a set of tools. It is designed to work with the OpenAI GPT-3.5-turbo model.\n\n`__init__.py` serves as an entry point for importing the `tool_bot` functionality, making it easy for other modules to access and use the `tool_bot` class and its associated methods. The `__all__` variable ensures that only the necessary names are imported when using wildcard imports.\n\n`bot.py` contains the `tool_bot` function, an asynchronous chatbot that listens for user input and processes it using a set of tools. It takes a list of tools, an optional prologue, user type, instruction, example, and a maximum number of iterations as input parameters. Here's an example of how the `tool_bot` function might be used:\n\n```python\ntools = [GetInformation, Calculate, Translate]\nprologue = \"Welcome to Turbo Chat!\"\nuser_type = \"customer\"\ninstruction = \"Ask me anything, and I'll try to help you using my available tools.\"\n\nawait tool_bot(tools, prologue, user_type, instruction)\n```\n\n`scratchpad.py` defines a data structure and a template for handling tools in the project. The `ParsedTools` class is a `TypedDict` that manages the parsed information related to tools, such as their names, inputs, and responses. The `scratchpad` variable is an instance of the `Scratchpad` class, which handles templates related to the `ParsedTools` data structure. Here's an example of how this code might be used:\n\n```python\nparsed_tools = ParsedTools(\n    should_use_tool=True,\n    tool_name=\"Example Tool\",\n    tool_input=\"Sample Input\",\n    final_response=\"Sample Output\"\n)\n\nformatted_output = scratchpad.format(parsed_tools)\nprint(formatted_output)\n```\n\n`template.py` defines a Jinja2 template for a chatbot conversation, where the chatbot acts as a facilitator between the user and an expert. The `TOOLBOT_TEMPLATE` generates a formatted conversation script, including optional prologue and additional information sections, instructions for the expert, and a list of available tools. The `EXAMPLE_TEMPLATE` wraps an example conversation between the expert and the user.\n\nIn the larger project, these components can be used to create a chatbot that assists users effectively using the available tools, manage and format the information related to tools, and generate conversation scripts for various user types and scenarios.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/tool/summary.md"}}],["15",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/bots/tool/template.py)\n\nThis code defines a template for a chatbot conversation in the Turbo-Chat project. The chatbot acts as a facilitator between the user and an expert, relaying messages between them and providing the expert with a set of tools to assist the user.\n\nThe `TOOLBOT_TEMPLATE` is a Jinja2 template that generates a formatted conversation script. It includes optional prologue and additional information sections, instructions for the expert, and a list of available tools. The template also specifies the format for user messages, expert responses, and tool usage.\n\nThe conversation starts with the chatbot introducing the user type and providing instructions to the expert. The expert's responses should be in the format `Response: <what you want to say>`. The chatbot will relay these responses to the user.\n\nThe expert has access to a set of tools, which are listed in the template. To use a tool, the expert must reply in the following format:\n\n```\n{{user_type | capitalize}} said: <what the {{user_type}} said>\nThought: Need to use a tool? <Yes or No>\nTool: <the name of the tool to use>\nTool Input: <the input to the tool in the format specified by the tool>\n```\n\nThe chatbot will then provide the result of the tool in the format `Tool Result: <the result of the tool you used>`.\n\nThe `EXAMPLE_TEMPLATE` is another Jinja2 template that wraps an example conversation between the expert and the user. It is used to provide examples of how the conversation should be conducted.\n\nIn the larger project, these templates can be used to generate conversation scripts for various user types and scenarios, helping the expert to assist users effectively using the available tools.\n## Questions: \n 1. **Question:** What is the purpose of the `TOOLBOT_TEMPLATE` and `EXAMPLE_TEMPLATE` variables in this code?\n   **Answer:** The `TOOLBOT_TEMPLATE` and `EXAMPLE_TEMPLATE` variables store string templates for generating a formatted conversation between the user and the toolbot. They are used to structure the conversation and provide instructions on how to interact with the toolbot and use the available tools.\n\n2. **Question:** How are the tools and their documentation added to the `Tools` section in the `TOOLBOT_TEMPLATE`?\n   **Answer:** The tools are added to the `Tools` section using a for loop `{% for tool in tools %}` that iterates through the `tools` list. For each tool, the tool's name (`{{tool.__name__}}`) and documentation (`{{tool.__doc__}}`) are added to the template.\n\n3. **Question:** How does the code handle the case when there is additional information to be provided in the `TOOLBOT_TEMPLATE`?\n   **Answer:** The code checks if the `additional_info` variable is present using `{% if additional_info -%}`. If it is present, the additional information is added to the template using `{{additional_info}}`.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/bots/tool/template.md"}}],["16",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/cache/__init__.py)\n\nThe code provided is part of a larger project, and it is responsible for importing and exposing a specific caching functionality, namely the `SimpleCache` class. This functionality is imported from a module named `simple` located within the same package as the current file.\n\nThe first line of the code is a comment that instructs `flake8`, a Python code linter, to ignore this file when checking for code style violations. This is done because the file uses a wildcard import (`*`), which is generally discouraged in Python as it can lead to unexpected behavior and make it harder to understand which names are being imported. However, in this case, the developer has explicitly chosen to use a wildcard import and wants to prevent `flake8` from raising any warnings about it.\n\nThe second line is a comment that mentions \"ruff\", which is likely a reference to a tool or process that removes unused imports. The comment indicates that the wildcard import should not be removed by this tool.\n\nThe actual code starts with the import statement, which imports everything from the `simple` module within the same package. This is done using the relative import syntax, denoted by the leading dot (`.`) before the module name.\n\nFinally, the `__all__` variable is defined as a list containing the string `\"SimpleCache\"`. This variable is used to specify which names should be imported when a client imports this module using a wildcard import. In this case, it ensures that only the `SimpleCache` class is exposed to the users of this module.\n\nIn the larger project, other modules can now import the `SimpleCache` class from this module as follows:\n\n```python\nfrom turbo_chat import SimpleCache\n\ncache = SimpleCache()\n```\n\nThis allows for a clean and simple interface to access the caching functionality provided by the `SimpleCache` class.\n## Questions: \n 1. **Question:** What is the purpose of the `# flake8: noqa` comment at the beginning of the code?\n   **Answer:** The `# flake8: noqa` comment is used to tell the Flake8 linter to ignore this file when checking for code style violations, as it may raise warnings for the wildcard import used in this file.\n\n2. **Question:** Why is the `*` import being used in this file, and what are the potential drawbacks of using such an import?\n   **Answer:** The `*` import is used to import all the names from the `simple` module into the current namespace. This can be useful for convenience, but it can also lead to potential issues such as name clashes and making it harder to trace where a specific name is coming from.\n\n3. **Question:** What is the purpose of the `__all__` variable in this code?\n   **Answer:** The `__all__` variable is used to define the public interface of this module, specifying which names should be imported when a client imports this module using a wildcard import (e.g., `from turbo_chat import *`). In this case, only the `SimpleCache` name is included in the public interface.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/cache/__init__.md"}}],["17",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/cache/simple.py)\n\nThe `SimpleCache` class in this code provides a simple in-memory caching mechanism for the Turbo-Chat project. It inherits from both `BaseCache` and `pydantic.BaseModel`, which means it utilizes the functionality provided by these two classes, such as data validation and serialization.\n\nThe `SimpleCache` class has a `cache` attribute, which is a dictionary that stores the cached data. It also has four asynchronous methods: `has`, `set`, `get`, and `clear`.\n\n- `has(key) -> bool`: This method checks if a given key exists in the cache. It returns `True` if the key is present, and `False` otherwise. For example:\n\n  ```python\n  cache = SimpleCache()\n  await cache.set(\"key1\", \"value1\")\n  print(await cache.has(\"key1\"))  # Output: True\n  print(await cache.has(\"key2\"))  # Output: False\n  ```\n\n- `set(key, value) -> None`: This method stores a key-value pair in the cache. The key is first converted to a string using the `to_key` method, and the value is serialized using the `serialize` method. For example:\n\n  ```python\n  cache = SimpleCache()\n  await cache.set(\"key1\", \"value1\")\n  await cache.set(\"key2\", {\"key\": \"value\"})\n  ```\n\n- `get(key) -> Any`: This method retrieves the value associated with a given key from the cache. It first checks if the key exists using the `has` method and raises an exception if the key is not found. Then, it returns the deserialized value using the `deserialize` method. For example:\n\n  ```python\n  cache = SimpleCache()\n  await cache.set(\"key1\", \"value1\")\n  print(await cache.get(\"key1\"))  # Output: \"value1\"\n  ```\n\n- `clear() -> Any`: This method clears the cache by resetting the `cache` attribute to an empty dictionary. For example:\n\n  ```python\n  cache = SimpleCache()\n  await cache.set(\"key1\", \"value1\")\n  await cache.clear()\n  print(await cache.has(\"key1\"))  # Output: False\n  ```\n\nIn the larger project, the `SimpleCache` class can be used to store and retrieve data that needs to be accessed frequently, reducing the need for expensive operations like database queries or API calls.\n## Questions: \n 1. **Question:** What is the purpose of the `SimpleCache` class and how does it work?\n   **Answer:** The `SimpleCache` class is a simple in-memory cache implementation that inherits from `BaseCache` and `pydantic.BaseModel`. It provides basic cache operations like checking if a key exists, setting a key-value pair, getting the value for a key, and clearing the cache.\n\n2. **Question:** How does the `to_key` method work and what is its purpose?\n   **Answer:** The `to_key` method is not shown in the provided code, but it is likely a method in the `BaseCache` class. Its purpose is to convert the given key into a format that can be used as a key in the cache dictionary.\n\n3. **Question:** What is the purpose of the `serialize` and `deserialize` methods used in the `set` and `get` methods?\n   **Answer:** The `serialize` and `deserialize` methods are not shown in the provided code, but they are likely methods in the `BaseCache` class. Their purpose is to convert the value into a storable format when setting a key-value pair in the cache and to convert it back to its original format when retrieving the value from the cache.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/cache/simple.md"}}],["18",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/tree/master/.autodoc/docs/json/turbo_chat/cache)\n\nThe `turbo_chat/cache` package provides a simple in-memory caching mechanism for the Turbo-Chat project. It consists of two files: `__init__.py` and `simple.py`.\n\n`__init__.py` is responsible for importing and exposing the `SimpleCache` class from the `simple` module. It uses a wildcard import to achieve this, and the `__all__` variable is defined to ensure that only the `SimpleCache` class is exposed to the users of this module. This allows for a clean and simple interface to access the caching functionality provided by the `SimpleCache` class.\n\n`simple.py` contains the `SimpleCache` class, which inherits from both `BaseCache` and `pydantic.BaseModel`. This means it utilizes the functionality provided by these two classes, such as data validation and serialization. The `SimpleCache` class has a `cache` attribute, which is a dictionary that stores the cached data, and four asynchronous methods: `has`, `set`, `get`, and `clear`.\n\nHere's an example of how the `SimpleCache` class can be used in the larger project:\n\n```python\nfrom turbo_chat import SimpleCache\n\ncache = SimpleCache()\n\n# Store a key-value pair in the cache\nawait cache.set(\"key1\", \"value1\")\n\n# Check if a key exists in the cache\nprint(await cache.has(\"key1\"))  # Output: True\n\n# Retrieve the value associated with a key from the cache\nprint(await cache.get(\"key1\"))  # Output: \"value1\"\n\n# Clear the cache\nawait cache.clear()\nprint(await cache.has(\"key1\"))  # Output: False\n```\n\nIn the larger project, the `SimpleCache` class can be used to store and retrieve data that needs to be accessed frequently, reducing the need for expensive operations like database queries or API calls.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/cache/summary.md"}}],["19",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/chat.py)\n\nThe `turbo-chat` code provides a chat runner function, `run_chat`, which is responsible for managing the conversation between the user and an AI assistant. The function takes a memory object, an optional cache object, and additional keyword arguments.\n\nThe main purpose of the `run_chat` function is to generate a response from the AI assistant based on the conversation history stored in the memory object. The function first retrieves the conversation history by calling the `prepare_prompt` method on the memory object. The conversation history is then used as a prompt for the AI model.\n\nBefore sending the prompt to the AI model, the function checks if the cache object is provided and if the cache has a response for the given prompt. If a cached response is found, it is returned as the AI assistant's response. This helps in reducing the response time and API calls to the AI model for previously encountered prompts.\n\nIf the prompt is not found in the cache, the function sends the prompt to the AI model using the `openai.ChatCompletion.acreate` method. The AI model generates a response, which is then parsed and converted into an `Assistant` object.\n\nAfter generating the response, the function appends the response to the memory object, ensuring that the conversation history is up-to-date. If a cache object is provided, the function also stores the generated response in the cache for future use.\n\nHere's an example of how the `run_chat` function can be used in the larger project:\n\n```python\nfrom turbo_chat import run_chat\nfrom turbo_chat.memory import Memory\nfrom turbo_chat.cache import Cache\n\nmemory = Memory()\ncache = Cache()\n\n# User input\nuser_message = \"What's the weather like today?\"\n\n# Add user message to memory\nmemory.append(user_message)\n\n# Get AI assistant's response\nassistant_response = await run_chat(memory, cache)\n\nprint(assistant_response.content)\n```\n\nIn summary, the `turbo-chat` code provides a chat runner function that manages the conversation between a user and an AI assistant, utilizing memory and cache objects to optimize the response generation process.\n## Questions: \n 1. **Question:** What is the purpose of the `run_chat` function and how does it work with memory and cache?\n\n   **Answer:** The `run_chat` function is responsible for running the ChatCompletion for the memory so far. It retrieves messages from memory, checks if the prompt is in the cache, and if not, creates a new completion using the OpenAI API. The result is then appended to memory and added to the cache if a cache is provided.\n\n2. **Question:** How does the `with_retries` decorator work with the `run_chat` function?\n\n   **Answer:** The `with_retries` decorator is used to handle retries in case of failures or exceptions while executing the `run_chat` function. It ensures that the function is executed multiple times (as specified in the decorator) until it succeeds or reaches the maximum number of retries.\n\n3. **Question:** What is the role of the `Assistant` class in the `run_chat` function, and how is it used?\n\n   **Answer:** The `Assistant` class is a data structure that represents the chat assistant's response. In the `run_chat` function, the `Assistant` class is used to store the output of the chat completion and return it as the result. It is also used to append the result to memory and add it to the cache if a cache is provided.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/chat.md"}}],["20",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/config.py)\n\nThis code defines the available models for the `turbo-chat` project, which is likely a chatbot application utilizing OpenAI's GPT models. The purpose of this code is to provide a list of allowed models and a type definition for the models that can be used throughout the project.\n\nThe `TurboModel` type is defined using the `Literal` type from the `typing` module. This type definition restricts the allowed values to a specific set of strings, which represent the names of the GPT models. The allowed model names are:\n\n- \"gpt-4\"\n- \"gpt-4-0314\"\n- \"gpt-4-32k\"\n- \"gpt-4-32k-0314\"\n- \"gpt-3.5-turbo\"\n- \"gpt-3.5-turbo-0301\"\n\nThe `available_models` variable is a list containing the same model names as defined in the `TurboModel` type. This list can be used in other parts of the project to iterate over the available models or to validate user input.\n\nThe `__all__` variable is a list containing the names of the public objects that should be imported when a client imports this module using a wildcard import (e.g., `from turbo_chat import *`). In this case, the `TurboModel` type and the `available_models` list are the only public objects.\n\nIn the larger project, this code can be used to ensure that only valid model names are used when interacting with the GPT models. For example, when initializing a chatbot instance, the project can check if the provided model name is in the `available_models` list and raise an error if it's not:\n\n```python\ndef create_chatbot(model_name: TurboModel):\n    if model_name not in available_models:\n        raise ValueError(f\"Invalid model name: {model_name}\")\n    # Initialize chatbot with the selected model\n```\n\nBy using the `TurboModel` type definition, the project can also benefit from type checking and autocompletion in IDEs, making it easier for developers to work with the code.\n## Questions: \n 1. **Question:** What is the purpose of the `TurboModel` type alias and how is it used in the code?\n\n   **Answer:** The `TurboModel` type alias is used to define a custom type that represents the allowed chatgpt model names. It is used in the code to specify the type of elements in the `available_models` list.\n\n2. **Question:** What is the purpose of the `__all__` variable and how does it affect the module's behavior?\n\n   **Answer:** The `__all__` variable is used to define the public interface of the module. It specifies which names should be imported when a client imports the module using a wildcard import (e.g., `from turbo_chat import *`).\n\n3. **Question:** What is the purpose of the `available_models` list and how can it be used by other modules?\n\n   **Answer:** The `available_models` list contains all the available chatgpt model names as defined by the `TurboModel` type alias. Other modules can use this list to access the available models and perform operations based on the supported models in the turbo-chat project.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/config.md"}}],["21",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/errors.py)\n\nThis code defines custom error classes and a placeholder class for handling specific situations related to generators in the `turbo-chat` project. Generators are a type of iterator that allows for lazy evaluation, meaning they only compute the next value when it is requested. They are particularly useful in situations where you need to process large amounts of data or when you want to create an infinite sequence.\n\nThe code starts by importing the `pydantic` library, which is a data validation and parsing library for Python. It is commonly used for creating data models and validating input data.\n\nNext, the `__all__` variable is defined, which is a list of public objects that should be imported when the module is imported using a wildcard (e.g., `from module import *`). In this case, the list includes two custom error classes: `InvalidValueYieldedError` and `GeneratorAlreadyExhaustedError`.\n\nThe `InvalidValueYieldedError` class is a custom error class that inherits from the built-in `ValueError` class. It is used to indicate that an invalid value was yielded by a generator. This error might be raised when the generator produces a value that does not meet certain criteria or is not expected by the application.\n\nThe `GeneratorAlreadyExhaustedError` class is another custom error class that inherits from the built-in `StopAsyncIteration` class. It is used to indicate that a generator has already been exhausted, meaning it has no more values to yield. This error might be raised when trying to iterate over a generator that has already reached its end.\n\nLastly, the `GeneratorAlreadyExhausted` class is a placeholder class that inherits from `pydantic.BaseModel`. This class is used to indicate that a generator has already been exhausted. It does not contain any additional functionality or attributes, but its presence in the codebase can be useful for signaling this specific situation to other parts of the application.\n\nIn summary, this code provides custom error handling and a placeholder class for dealing with generators in the `turbo-chat` project. These classes can be used to handle specific situations related to generators, such as invalid values being yielded or generators being exhausted.\n## Questions: \n 1. **Question:** What is the purpose of the `__all__` variable in this code?\n   **Answer:** The `__all__` variable is used to define the public interface of this module. It specifies which names should be imported when a client imports this module using a wildcard import (e.g., `from turbo_chat import *`).\n\n2. **Question:** Why are there two classes with similar names: `GeneratorAlreadyExhaustedError` and `GeneratorAlreadyExhausted`?\n   **Answer:** `GeneratorAlreadyExhaustedError` is an exception class that inherits from `StopAsyncIteration`, while `GeneratorAlreadyExhausted` is a placeholder class that inherits from `pydantic.BaseModel`. They serve different purposes: the former is used to raise an exception when the generator is exhausted, and the latter is a placeholder value to indicate that the generator was already exhausted.\n\n3. **Question:** What is the purpose of the `...` (ellipsis) in the class definitions?\n   **Answer:** The ellipsis `...` is used as a placeholder for the class body. In this case, it indicates that the classes have no additional methods or attributes, and their main purpose is to serve as custom exception or placeholder classes with specific names and inheritance.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/errors.md"}}],["22",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/memory/__init__.py)\n\nThis code is responsible for managing different types of memory storage within the turbo-chat project. It imports three different memory storage classes from their respective modules and makes them available for use in other parts of the project. The three memory storage classes are:\n\n1. `LocalMemory`: This class provides a basic local memory storage implementation. It can be used to store and manage chat data within the local environment of the application.\n\n2. `LocalTruncatedMemory`: This class extends the functionality of `LocalMemory` by implementing a truncated memory storage. It is designed to store only a limited amount of chat data, discarding older data when the storage limit is reached. This can be useful for managing memory usage in situations where only recent chat data is relevant.\n\n3. `LocalSummarizeMemory`: This class also extends `LocalMemory`, but it focuses on providing a summarized view of the chat data. It can be used to generate a condensed version of the chat history, which can be useful for providing an overview of the conversation or for generating reports.\n\nThe code also defines a list called `__all__`, which explicitly specifies the classes that should be imported when using a wildcard import statement (e.g., `from memory_storage import *`). This helps to keep the namespace clean and prevent unintended imports.\n\nHere's an example of how these classes might be used in the larger project:\n\n```python\nfrom memory_storage import LocalMemory, LocalTruncatedMemory, LocalSummarizeMemory\n\n# Create instances of the different memory storage classes\nbasic_memory = LocalMemory()\ntruncated_memory = LocalTruncatedMemory()\nsummarized_memory = LocalSummarizeMemory()\n\n# Store chat data in the different memory storage instances\nbasic_memory.store_chat_data(chat_data)\ntruncated_memory.store_chat_data(chat_data)\nsummarized_memory.store_chat_data(chat_data)\n\n# Retrieve and process chat data from the different memory storage instances\nbasic_chat_data = basic_memory.get_chat_data()\ntruncated_chat_data = truncated_memory.get_chat_data()\nsummarized_chat_data = summarized_memory.get_chat_data()\n```\n\nBy providing different memory storage implementations, this code allows the turbo-chat project to easily switch between different storage strategies depending on the requirements of the application.\n## Questions: \n 1. **Question:** What is the purpose of the `# flake8: noqa` comment at the beginning of the code?\n   **Answer:** The `# flake8: noqa` comment is used to tell Flake8, a Python code linter, to ignore this file when checking for code style violations, such as the use of wildcard imports (`*`).\n\n2. **Question:** What are the different memory classes being imported from the three modules, and how do they differ in functionality?\n   **Answer:** The code imports three memory classes from their respective modules: `LocalMemory` from `local_memory`, `LocalTruncatedMemory` from `truncated_memory`, and `LocalSummarizeMemory` from `summary_memory`. Each class likely represents a different memory management strategy for the turbo-chat project, but the specific differences in functionality would need to be determined by examining the respective module files.\n\n3. **Question:** Why is the `__all__` variable defined at the end of the code, and what is its purpose?\n   **Answer:** The `__all__` variable is a list that defines the public interface of the module, specifying which names should be imported when a client imports the module using a wildcard import (`from module import *`). In this case, it includes the three memory classes that are intended to be part of the module's public API.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/memory/__init__.md"}}],["23",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/memory/local_memory.py)\n\nThe `LocalMemory` class in this code is an implementation of the `BaseMemory` abstract class, designed to store messages and state information for the Turbo-Chat project. It provides an in-memory storage solution for messages and state data, which can be useful for testing or small-scale applications where persistence is not required.\n\nThe class has two main attributes: `state` and `messages`. The `state` attribute is a dictionary that stores the current state of the chat, while the `messages` attribute is a list that holds `Message` objects.\n\nThere are four main methods in the `LocalMemory` class:\n\n1. `get`: This asynchronous method returns a list of all messages stored in the `messages` attribute. It can be used to retrieve the chat history.\n\n   Example usage:\n   ```python\n   local_memory = LocalMemory()\n   chat_history = await local_memory.get()\n   ```\n\n2. `extend`: This asynchronous method takes a list of messages as input and appends them to the `messages` attribute. It can be used to add new messages to the chat.\n\n   Example usage:\n   ```python\n   new_messages = [Message(...), Message(...)]\n   await local_memory.extend(new_messages)\n   ```\n\n3. `get_state`: This asynchronous method returns the current state of the chat as a dictionary. It can be used to retrieve the chat state for processing or display purposes.\n\n   Example usage:\n   ```python\n   chat_state = await local_memory.get_state()\n   ```\n\n4. `set_state`: This asynchronous method takes a new state dictionary as input and updates the `state` attribute. It also has an optional `merge` parameter, which, if set to `True`, merges the new state with the existing state instead of replacing it.\n\n   Example usage:\n   ```python\n   new_state = {\"key\": \"value\"}\n   await local_memory.set_state(new_state, merge=True)\n   ```\n\nOverall, the `LocalMemory` class provides a simple in-memory storage solution for messages and state data in the Turbo-Chat project, which can be useful for testing or small-scale applications.\n## Questions: \n 1. **Question:** What is the purpose of the `LocalMemory` class and how does it store messages?\n   \n   **Answer:** The `LocalMemory` class is an implementation of the `BaseMemory` abstract class, and its purpose is to store messages in an in-memory list. It uses the `messages` attribute, which is a list of `Message` objects, to store the messages.\n\n2. **Question:** How does the `set_state` method work and what is the purpose of the `merge` parameter?\n\n   **Answer:** The `set_state` method is used to update the state of the `LocalMemory` instance. The `merge` parameter is a boolean flag that determines whether the new state should be merged with the existing state (if `True`) or completely replace the existing state (if `False`).\n\n3. **Question:** Are there any concurrency issues that might arise from using the `LocalMemory` class in a multi-threaded or asynchronous environment?\n\n   **Answer:** Since the `LocalMemory` class uses in-memory storage (lists and dictionaries) and does not implement any locking mechanisms, there might be concurrency issues when using this class in a multi-threaded or asynchronous environment. It is important to ensure proper synchronization when accessing and modifying the `messages` and `state` attributes in such scenarios.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/memory/local_memory.md"}}],["24",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/tree/master/.autodoc/docs/json/turbo_chat/memory)\n\nThe `turbo-chat` project provides a memory management system with various storage strategies for handling chat data. The memory management system is located in the `.autodoc/docs/json/turbo_chat/memory` folder and consists of several classes that offer different storage and processing capabilities.\n\nThe `LocalMemory` class provides a basic in-memory storage solution for messages and state data. It can be used for testing or small-scale applications where persistence is not required. Example usage:\n\n```python\nlocal_memory = LocalMemory()\nchat_history = await local_memory.get()\n```\n\nThe `LocalTruncatedMemory` class extends `LocalMemory` and implements a truncated memory storage. It ensures that the total number of tokens in the chat history does not exceed the model's context window. This can be useful for managing memory usage in situations where only recent chat data is relevant. Example usage:\n\n```python\nmemory = LocalTruncatedMemory(model=TurboModel)\ntruncated_prompt = await memory.prepare_prompt(max_tokens=100)\n```\n\nThe `LocalSummarizeMemory` class also extends `LocalMemory` and focuses on providing a summarized view of the chat data. It can be used to generate a condensed version of the chat history, which can be useful for providing an overview of the conversation or for generating reports. Example usage:\n\n```python\nsummarized_memory = LocalSummarizeMemory()\nsummarized_chat_data = await summarized_memory.prepare_prompt()\n```\n\nBy providing different memory storage implementations, the code allows the turbo-chat project to easily switch between different storage strategies depending on the requirements of the application.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/memory/summary.md"}}],["25",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/memory/summary_memory.py)\n\nThe `turbo-chat` code provided defines a memory management system with automatic summarization capabilities. This system is designed to handle and summarize conversation messages in the larger project. The code consists of two main classes: `MemorySummarization` and `LocalSummarizeMemory`.\n\n`MemorySummarization` is a mixin class that inherits from `MemoryTruncation`. It provides an asynchronous method `prepare_prompt` that takes an optional `max_tokens` parameter. The purpose of this method is to summarize the conversation by extracting the first, middle, and last messages. If there are no middle messages, the original messages are returned. Otherwise, the middle conversation is summarized using the `summarize_bot` function, which is imported from the `..bots` module. The summarized text is then converted into a `User` object and returned as a list containing the first message, the summary, and the last message.\n\n```python\nsummary = await summarize_bot(text=conversation, text_type=\"conversation\").run()\nsummary_as_user = User(summary.content).dict()\nreturn [first, summary_as_user, last]\n```\n\n`LocalSummarizeMemory` is a class that inherits from both `MemorySummarization` and `LocalMemory`. This class combines the functionality of local memory storage with the automatic summarization provided by the `MemorySummarization` mixin. It does not define any additional methods or attributes, but it serves as a concrete implementation of the summarization functionality.\n\n```python\nclass LocalSummarizeMemory(MemorySummarization, LocalMemory):\n    \"\"\"Local memory with automatic summarization\"\"\"\n    ...\n```\n\nIn the larger project, the `LocalSummarizeMemory` class can be used to store and manage conversation messages while providing automatic summarization capabilities. This can help reduce the amount of text that needs to be processed or displayed, making the conversation more manageable and efficient.\n## Questions: \n 1. **Question:** What is the purpose of the `MemorySummarization` class and how does it work?\n   **Answer:** The `MemorySummarization` class is a mixin for automatic summarization of the conversation. It provides an implementation of the `prepare_prompt` method that summarizes the middle part of the conversation using the `summarize_bot`.\n\n2. **Question:** How does the `LocalSummarizeMemory` class utilize the `MemorySummarization` mixin?\n   **Answer:** The `LocalSummarizeMemory` class inherits from both `MemorySummarization` and `LocalMemory` classes, combining their functionalities. This allows it to have local memory storage while also providing automatic summarization of the conversation.\n\n3. **Question:** How does the `prepare_prompt` method in the `MemorySummarization` class handle different conversation lengths?\n   **Answer:** The `prepare_prompt` method first checks if there are any middle messages in the conversation. If there are no middle messages, it returns the original conversation without summarization. If there are middle messages, it summarizes the middle part of the conversation and returns the first message, the summary, and the last message.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/memory/summary_memory.md"}}],["26",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/memory/truncated_memory.py)\n\nThe `turbo-chat` code provided defines a memory truncation mechanism for a chatbot model. This mechanism ensures that the total number of tokens in the chat history does not exceed the model's context window, which is the maximum number of tokens the model can process at once. The code defines two classes: `MemoryTruncation` and `LocalTruncatedMemory`.\n\n`MemoryTruncation` is a mixin class that provides an automatic truncation feature. It has a single method, `prepare_prompt`, which takes an optional `max_tokens` parameter. The method first retrieves the chat history (messages) by calling the `prepare_prompt` method of its superclass. It then ensures that the first message in the chat history is within the context window limit. If there are more messages, it checks if the combined length of the first and last messages is within the context window limit.\n\nThe method then iteratively filters the middle messages by removing them from the end until the total number of tokens is within the context window limit. The filtered chat history is returned as a list of `MessageDict` objects.\n\n`LocalTruncatedMemory` is a subclass of both `MemoryTruncation` and `LocalMemory`. It inherits the automatic truncation feature from `MemoryTruncation` and the local memory storage functionality from `LocalMemory`. This class can be used to store and manage the chat history while ensuring that the total number of tokens does not exceed the model's context window.\n\nIn the larger project, `LocalTruncatedMemory` can be used to manage the chatbot's memory efficiently. When preparing a prompt for the chatbot model, the `prepare_prompt` method can be called to get a truncated chat history that fits within the model's context window. This ensures that the chatbot can process the chat history without running into token limit issues.\n\nExample usage:\n\n```python\nmemory = LocalTruncatedMemory(model=TurboModel)\n# Add messages to memory\nmemory.add_message(...)\n# Prepare a prompt with truncated chat history\ntruncated_prompt = await memory.prepare_prompt(max_tokens=100)\n```\n## Questions: \n 1. **Question:** What is the purpose of the `MemoryTruncation` mixin class?\n   **Answer:** The `MemoryTruncation` mixin class provides an automatic truncation functionality for memory classes. It ensures that the total number of tokens in the messages does not exceed the context window limit by iteratively dropping messages while keeping the first and last messages.\n\n2. **Question:** How does the `prepare_prompt` method work in the `MemoryTruncation` class?\n   **Answer:** The `prepare_prompt` method takes an optional `max_tokens` parameter and calculates the context window limit based on the model's maximum tokens length. It then retrieves the messages and ensures that the first message and the combination of the first and last messages are within the context window limit. Finally, it iteratively filters the middle messages by removing them from the end until the total tokens count is within the context window limit.\n\n3. **Question:** What is the purpose of the `LocalTruncatedMemory` class?\n   **Answer:** The `LocalTruncatedMemory` class is a combination of the `MemoryTruncation` mixin and the `LocalMemory` class. It provides a local memory implementation with the automatic truncation functionality from the `MemoryTruncation` mixin.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/memory/truncated_memory.md"}}],["27",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/runner.py)\n\nThe `turbo-chat` code provided is responsible for running a Turbo application using asynchronous generators. The main function in this code is `run`, which takes two arguments: `gen` and `input`. The `gen` argument is of type `TurboGenWrapper`, which is a custom type hint for the asynchronous generator. The `input` argument is optional and can be either a string or a dictionary.\n\nThe purpose of the `run` function is to execute the given asynchronous generator (`gen`) and return a `Result` object, which contains information about the generator's output and whether it has been exhausted or not.\n\nThe function starts by setting placeholder values for `done` and `output`. The `done` variable is a boolean flag that indicates if the generator has been exhausted, while `output` is an instance of the `GeneratorAlreadyExhausted` class.\n\nThe main logic of the function is enclosed in a `try` block, where the generator is run using the `asend` method in a loop until it requires input or is exhausted. If the generator is exhausted, a `StopAsyncIteration` exception is raised, and the `done` flag is set to `True`.\n\nAfter the loop, the function checks if the `output` is still an instance of `GeneratorAlreadyExhausted`. If it is, this means the generator has not produced any output, and a `GeneratorAlreadyExhaustedError` is raised.\n\nFinally, the function casts the `output` to a `Result` object, sets its `done` attribute to the value of the `done` flag, and returns the `Result` object.\n\nIn the larger project, this code can be used to run Turbo applications that rely on asynchronous generators. The `run` function provides a convenient way to execute these generators and handle their output, as well as manage their exhaustion state. For example, the following code snippet demonstrates how to use the `run` function:\n\n```python\nfrom turbo_chat import TurboGenWrapper, run\n\nasync def my_generator():\n    # Generator logic here\n\ngen = TurboGenWrapper(my_generator())\nresult = await run(gen)\nprint(result.output)\n```\n## Questions: \n 1. **Question:** What is the purpose of the `TurboGenWrapper` type and how is it used in the `run` function?\n   **Answer:** The `TurboGenWrapper` type is not defined in this code snippet, but it is likely a custom wrapper around an asynchronous generator. In the `run` function, it is used as the input parameter `gen`, which is then used in the `while` loop to asynchronously send input and receive output from the generator.\n\n2. **Question:** What is the role of the `GeneratorAlreadyExhausted` class and how is it used in this code?\n   **Answer:** The `GeneratorAlreadyExhausted` class is an exception class that is used as a placeholder for the `output` variable initially. If the generator is exhausted and the output is still an instance of `GeneratorAlreadyExhausted`, the `GeneratorAlreadyExhaustedError` is raised, indicating that the generator has already been exhausted and cannot be used further.\n\n3. **Question:** How does the `Result` struct work and what is its purpose in the `run` function?\n   **Answer:** The `Result` struct is not defined in this code snippet, but it is likely a custom data structure used to store the output of the generator and the `done` status. In the `run` function, the `output` is cast to a `Result` type, and the `done` status is set before returning the `result` object.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/runner.md"}}],["28",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/structs/__init__.py)\n\nThe code provided is part of a larger project called `turbo-chat`. It primarily serves as a module that imports and exports various components related to chat functionality. The purpose of this module is to make it easier for other parts of the project to access and use these components by providing a single point of import.\n\nThe code starts by disabling flake8 linting for wildcard imports, which is done to prevent the removal of such imports by the `ruff` tool. Following this, several components are imported from their respective modules:\n\n1. `messages`: This module likely contains classes and functions related to handling chat messages.\n2. `result`: This module may contain classes and functions for processing and returning results from chat interactions.\n3. `scratchpad`: The `Scratchpad` class is imported, which could be a utility class for storing and managing temporary data during chat sessions.\n4. `signals`: This module might contain classes and functions for handling events and signals during chat interactions.\n\nAfter importing the necessary components, the code defines an `__all__` list, which explicitly specifies the public interface of this module. This list includes various classes and functions that are expected to be used by other parts of the project:\n\n- `System`, `User`, `Assistant`: These classes might represent different roles or entities in the chat system.\n- `ExampleUser`, `ExampleAssistant`: These classes could be example implementations or subclasses of the `User` and `Assistant` classes, respectively.\n- `Generate`, `GetInput`: These functions might be responsible for generating chat messages and obtaining user input.\n- `Example`: This class or function could serve as a demonstration or template for implementing chat functionality.\n- `Scratchpad`: As mentioned earlier, this class might be a utility for managing temporary data during chat sessions.\n- `Result`: This class or function could be responsible for processing and returning results from chat interactions.\n\nIn summary, this code serves as a module that imports and exports various components related to chat functionality in the `turbo-chat` project. By providing a single point of import, it simplifies the process of accessing and using these components in other parts of the project.\n## Questions: \n 1. **Question:** What is the purpose of the `# flake8: noqa` comment at the beginning of the code?\n   **Answer:** The `# flake8: noqa` comment is used to tell the Flake8 linter to ignore this file when checking for code style violations, such as the use of wildcard imports.\n\n2. **Question:** What are the `*` imports being used for in this code?\n   **Answer:** The `*` imports are used to import all the names from the specified modules (messages, result, scratchpad, and signals) into the current namespace, making them available for use in this module.\n\n3. **Question:** What is the purpose of the `__all__` list in this code?\n   **Answer:** The `__all__` list is used to define the public interface of this module, specifying which names should be imported when a client imports this module using a wildcard import (e.g., `from turbo_chat import *`).","metadata":{"source":".autodoc/docs/markdown/turbo_chat/structs/__init__.md"}}],["29",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/structs/messages.py)\n\nThis code defines various message types and an example message collection for the Turbo-Chat project. The purpose of this code is to provide a structure for different types of messages that can be used in the chat application, such as system messages, user messages, and assistant messages.\n\nThe code starts by importing necessary modules and types, such as `List` from `typing`, `pydantic`, and `BaseMessageCollection`, `MessageRole`, and `Message` from `..types.messages`. It then defines the exported names using the `__all__` variable.\n\nThere are five message classes defined in this code:\n\n1. `System`: Represents a system message with a fixed role of \"system\".\n2. `User`: Represents a user message with a fixed role of \"user\".\n3. `Assistant`: Represents an assistant message with a fixed role of \"assistant\" and an additional attribute `forward` set to `True`.\n4. `ExampleUser`: Represents a user example message with a fixed role of \"system name=example_user\".\n5. `ExampleAssistant`: Represents an assistant example message with a fixed role of \"system name=example_assistant\".\n\nAll these classes inherit from the `Message` class, which is imported from `..types.messages`.\n\nThe code also defines an abstract implementation called `Example`, which inherits from `BaseMessageCollection` and `pydantic.BaseModel`. This class has two attributes, `user` and `assistant`, and an asynchronous method `get()` that returns a list of `Message` objects. The `get()` method creates an `ExampleUser` message with the content of `self.user` and an `ExampleAssistant` message with the content of `self.assistant`.\n\nIn the larger project, these message classes can be used to create and manage different types of messages in the chat application. For example, when a user sends a message, an instance of the `User` class can be created with the appropriate content. Similarly, when the assistant responds, an instance of the `Assistant` class can be created. The `Example` class can be used to create example message collections for testing or demonstration purposes.\n## Questions: \n 1. **Question:** What is the purpose of the `forward` attribute in the `Assistant` class?\n   **Answer:** The `forward` attribute in the `Assistant` class is a boolean flag that indicates whether the assistant message should be forwarded or not. Its default value is set to `True`.\n\n2. **Question:** How does the `Example` class work and what is its relationship with the `BaseMessageCollection` and `pydantic.BaseModel`?\n   **Answer:** The `Example` class inherits from both `BaseMessageCollection` and `pydantic.BaseModel`. It represents an example message collection containing a user message and an assistant message. The `get` method returns a list of `ExampleUser` and `ExampleAssistant` instances with the provided content.\n\n3. **Question:** What is the significance of the `__all__` variable in the code?\n   **Answer:** The `__all__` variable is a list that defines the public interface of the module. It specifies which names should be imported when a client imports the module using a wildcard import (e.g., `from module import *`). In this case, it includes various message classes, roles, and the `BaseMessageCollection` class.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/structs/messages.md"}}],["30",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/structs/proxies.py)\n\nThe `turbo-chat` code provided defines a `TurboGenWrapper` class and a `proxy_turbo_gen_fn` function. The main purpose of this code is to create a proxy wrapper around generator functions in the larger project, allowing them to be used asynchronously and providing a consistent interface for running them.\n\nThe `TurboGenWrapper` class inherits from `ObjectWrapper` and adds two methods: `__aiter__` and `run`. The `__aiter__` method allows the wrapped generator to be used as an asynchronous iterator, enabling the use of `async for` loops with the generator. The `run` method is an asynchronous function that takes an optional input of type `str` or `dict` and runs the wrapped generator using the `gen_run` function from the `runner` module. This method is useful for executing the generator with a specific input and handling the result asynchronously.\n\nThe `proxy_turbo_gen_fn` function is a decorator that takes a generator function as an argument and returns a wrapped version of the function. When the wrapped function is called, it creates an instance of the `TurboGenWrapper` class with the generator and returns the proxy object. This allows the generator to be used with the additional functionality provided by the `TurboGenWrapper` class.\n\nHere's an example of how this code might be used in the larger project:\n\n```python\n@proxy_turbo_gen_fn\ndef my_generator(input):\n    # Generator logic here\n    yield result\n\nasync def main():\n    # Create an instance of the wrapped generator\n    wrapped_gen = my_generator(\"some input\")\n\n    # Use the wrapped generator as an async iterator\n    async for result in wrapped_gen:\n        # Process the result asynchronously\n        pass\n\n    # Run the wrapped generator with a specific input\n    result = await wrapped_gen.run(\"another input\")\n```\n\nIn summary, the provided code defines a proxy wrapper for generator functions in the `turbo-chat` project, enabling them to be used asynchronously and providing a consistent interface for running them with specific inputs.\n## Questions: \n 1. **Question:** What is the purpose of the `TurboGenWrapper` class and how does it extend the functionality of the `ObjectWrapper` class from the `peak.util.proxies` module?\n\n   **Answer:** The `TurboGenWrapper` class is a custom wrapper for generator objects, extending the functionality of the `ObjectWrapper` class by adding an asynchronous iterator method (`__aiter__`) and an asynchronous `run` method. This allows the wrapped generator to be used in asynchronous contexts.\n\n2. **Question:** How does the `proxy_turbo_gen_fn` function work and what is its intended use?\n\n   **Answer:** The `proxy_turbo_gen_fn` function is a decorator that takes a generator function as input and returns a wrapped version of the function. When the wrapped function is called, it creates a generator object, wraps it with a `TurboGenWrapper` instance, and returns the wrapped generator. This allows the generator to be used with the additional functionality provided by the `TurboGenWrapper` class.\n\n3. **Question:** In the `run` method of the `TurboGenWrapper` class, why is the import statement for `gen_run` placed inside the method instead of at the top of the file?\n\n   **Answer:** The import statement for `gen_run` is placed inside the `run` method to avoid a circular import issue. This means that importing the `gen_run` function at the top of the file would cause a circular dependency between the modules, which can lead to errors or unexpected behavior. By importing it inside the method, the circular dependency is avoided.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/structs/proxies.md"}}],["31",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/structs/result.py)\n\nThe code provided is part of the `turbo-chat` project and defines a `Result` class that holds the result yielded by a turbo app. The purpose of this class is to store the content of a message, whether the message requires user input, and whether the message processing is done.\n\nThe code starts by importing necessary modules and defining the `__all__` variable, which is a list containing the names of public objects that should be imported when the module is imported using a wildcard import statement.\n\nNext, a `HasContent` protocol is defined. This protocol specifies that any class implementing it should have a `content` attribute of type `str`. This protocol is used later in the `Result` class to ensure that the message object passed to the `from_message` method has a `content` attribute.\n\nThe `Result` class is a subclass of `pydantic.BaseModel`, which is a data validation and parsing library. The class has three attributes:\n\n1. `content`: This attribute stores the content of the message and can be of any type.\n2. `needs_input`: This boolean attribute indicates whether the message requires user input. It defaults to `False`.\n3. `done`: This boolean attribute indicates whether the message processing is done. It defaults to `False`.\n\nThe `Result` class also has a class method called `from_message`, which takes a `message` object and an optional `done` boolean parameter. This method first checks if the `message` object has a `content` attribute, as required by the `HasContent` protocol. If the check passes, it creates and returns a new `Result` instance with the `content` attribute set to the `content` of the message, the `needs_input` attribute set to `True` if the message is an instance of `GetInput`, and the `done` attribute set to the value of the `done` parameter.\n\nHere's an example of how the `Result` class might be used in the larger project:\n\n```python\n# Create a message object with content\nmessage = SomeMessage(content=\"Hello, world!\")\n\n# Create a Result instance from the message object\nresult = Result.from_message(message, done=True)\n\n# Check if the result needs input and if it's done\nif result.needs_input:\n    # Get user input and process it\n    pass\nelif result.done:\n    # Handle the completed result\n    pass\n```\n## Questions: \n 1. **Question:** What is the purpose of the `HasContent` protocol in this code?\n   **Answer:** The `HasContent` protocol is used to define a type that requires a `content` attribute of type `str`. This protocol is later used as a type hint for the `message` parameter in the `from_message` class method of the `Result` class.\n\n2. **Question:** How does the `from_message` class method work and what does it return?\n   **Answer:** The `from_message` class method takes a `message` object of type `HasContent` and an optional `done` boolean parameter. It checks if the `message` object has a `content` attribute and then creates and returns a new `Result` object with the `content` attribute set to the `message.content`, `needs_input` attribute set to `True` if the `message` is an instance of `GetInput`, and `done` attribute set to the provided `done` value.\n\n3. **Question:** What is the purpose of the `Result` class and its attributes?\n   **Answer:** The `Result` class is used to hold the result yielded by a turbo app. It has three attributes: `content` which can be of any type and holds the content of the result, `needs_input` which is a boolean indicating if the result requires input from the user, and `done` which is a boolean indicating if the processing is completed.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/structs/result.md"}}],["32",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/structs/scratchpad.py)\n\nThe `turbo-chat` project contains a `Scratchpad` class that is designed to parse input strings according to a given specification. The purpose of this class is to extract structured data from unstructured text input, which can be useful in a chat application for processing user messages or commands.\n\nThe `Scratchpad` class is a generic class that takes a type variable `ST` as a parameter. This allows the class to be used with different types of structured data, making it more versatile and reusable.\n\nThe class has two main methods: `_search` and `parse`. The `_search` method is a private helper method that uses the `parse.search` function from the `parse` library to search for a given pattern in the input string. It takes a specification string and an input string as arguments and returns the parsed result.\n\nThe `parse` method is the main method of the class, which takes an input string and returns the parsed result as an instance of the type variable `ST`. It first splits the specification string into a list of line specifications, then iterates through the list and calls the `_search` method for each line specification. The parsed results are then combined into a single dictionary, which is cast to the type variable `ST` before being returned.\n\nIn addition to the `Scratchpad` class, the code also defines a `parse_yesno` function, which is a custom parser for boolean values. This function uses a predefined mapping of string values to their corresponding boolean values (e.g., \"yes\" maps to `True`, \"no\" maps to `False`, etc.). The `parse_yesno` function is decorated with the `with_pattern` decorator from the `parse` library, which allows it to be used as a custom parser in the `parse.search` function.\n\nHere's an example of how the `Scratchpad` class might be used in the `turbo-chat` project:\n\n```python\nspec = \"Name: {name}\\nAge: {age:d}\\nActive: {active:bool}\"\ninput_str = \"Name: John\\nAge: 25\\nActive: yes\"\n\nscratchpad = Scratchpad(spec)\nresult = scratchpad.parse(input_str)\n\nprint(result)  # Output: {'name': 'John', 'age': 25, 'active': True}\n```\n\nIn this example, the `Scratchpad` class is used to parse a user's input string containing their name, age, and active status. The parsed result is a dictionary containing the extracted data.\n## Questions: \n 1. **Question**: What is the purpose of the `yesno_mapping` dictionary and how is it used in the `parse_yesno` function?\n   **Answer**: The `yesno_mapping` dictionary is used to map various string representations of boolean values (e.g., \"yes\", \"no\", \"on\", \"off\", etc.) to their corresponding boolean values (True or False). The `parse_yesno` function takes a string input and returns the corresponding boolean value by looking it up in the `yesno_mapping` dictionary.\n\n2. **Question**: How does the `Scratchpad` class utilize the `_search` method and what is its role in the parsing process?\n   **Answer**: The `Scratchpad` class uses the `_search` method to parse the input string according to the given spec. The method utilizes the `parse.search` function from the `parse` library to perform the parsing, and it is called for each line spec in the `parse` method of the `Scratchpad` class.\n\n3. **Question**: What is the purpose of the `ST` TypeVar and how is it used in the `Scratchpad` class?\n   **Answer**: The `ST` TypeVar is a generic type variable that is used to represent the type of the parsed result in the `Scratchpad` class. It is used as a type hint for the return type of the `parse` method, allowing the developer to specify the expected type of the parsed result when using the `Scratchpad` class.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/structs/scratchpad.md"}}],["33",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/structs/signals.py)\n\nThis code is responsible for handling signals related to user input and completion generation in the Turbo-Chat project. It utilizes the Pydantic library to define two classes, `Generate` and `GetInput`, which are used as placeholders for specific actions within the chat application.\n\nThe `Generate` class is a Pydantic `BaseModel` that allows extra attributes to be added to the model. It has a single attribute, `forward`, which is a boolean value set to `True` by default. This class is used as a placeholder to indicate that the completion generation process should be run. For example, when the chat application needs to generate a response or suggestion based on user input, it can create an instance of the `Generate` class and pass it to the appropriate function or method.\n\n```python\ngenerate_signal = Generate()\ncompletion = some_function(generate_signal)\n```\n\nThe `GetInput` class is another Pydantic `BaseModel` with a single attribute, `content`, which is a string set to \"User input needed\" by default. This class is used as a placeholder to indicate that user input is required at a specific point in the chat application. For instance, when the application needs to prompt the user for input, it can create an instance of the `GetInput` class and pass it to the appropriate function or method.\n\n```python\nget_input_signal = GetInput()\nuser_input = some_function(get_input_signal)\n```\n\nBoth classes are included in the `__all__` list, which defines the public interface of this module. This allows other parts of the Turbo-Chat project to import and use these classes as needed.\n\nIn summary, this code provides a way for the Turbo-Chat project to handle user input and completion generation using Pydantic models as placeholders for specific actions. This approach enables a clean and structured way to manage these processes within the larger project.\n## Questions: \n 1. **Question:** What is the purpose of the `extra` parameter in the `Generate` class definition?\n\n   **Answer:** The `extra` parameter in the `Generate` class definition allows the class to accept additional, unexpected fields in the input data without raising a validation error. In this case, it is set to `pydantic.Extra.allow`, which means any extra fields will be allowed and included in the model.\n\n2. **Question:** What is the role of the `__all__` variable in this code?\n\n   **Answer:** The `__all__` variable is used to define the public interface of the module. It is a list of strings that specifies which names should be imported when a client imports the module using a wildcard import (e.g., `from turbo_chat import *`). In this case, only the `Generate` and `GetInput` classes are part of the public interface.\n\n3. **Question:** What is the purpose of the `content` attribute in the `GetInput` class?\n\n   **Answer:** The `content` attribute in the `GetInput` class is used to store a string representing the user input that is needed. By default, it is set to \"User input needed\", which serves as a placeholder value to indicate that user input is required.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/structs/signals.md"}}],["34",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/tree/master/.autodoc/docs/json/turbo_chat/structs)\n\nThe `turbo-chat/structs` folder contains code related to chat functionality, such as handling messages, processing results, and managing temporary data during chat sessions. It provides a single point of import for these components, simplifying their usage in other parts of the project.\n\n`messages.py` defines various message types and an example message collection. These classes can be used to create and manage different types of messages in the chat application, such as system messages, user messages, and assistant messages. The `Example` class can be used to create example message collections for testing or demonstration purposes.\n\n`proxies.py` defines a `TurboGenWrapper` class and a `proxy_turbo_gen_fn` function, which create a proxy wrapper around generator functions, allowing them to be used asynchronously and providing a consistent interface for running them. For example:\n\n```python\n@proxy_turbo_gen_fn\ndef my_generator(input):\n    # Generator logic here\n    yield result\n\nasync def main():\n    # Create an instance of the wrapped generator\n    wrapped_gen = my_generator(\"some input\")\n\n    # Use the wrapped generator as an async iterator\n    async for result in wrapped_gen:\n        # Process the result asynchronously\n        pass\n\n    # Run the wrapped generator with a specific input\n    result = await wrapped_gen.run(\"another input\")\n```\n\n`result.py` defines a `Result` class that holds the result yielded by a turbo app. It stores the content of a message, whether the message requires user input, and whether the message processing is done. For example:\n\n```python\n# Create a message object with content\nmessage = SomeMessage(content=\"Hello, world!\")\n\n# Create a Result instance from the message object\nresult = Result.from_message(message, done=True)\n\n# Check if the result needs input and if it's done\nif result.needs_input:\n    # Get user input and process it\n    pass\nelif result.done:\n    # Handle the completed result\n    pass\n```\n\n`scratchpad.py` contains a `Scratchpad` class that parses input strings according to a given specification, extracting structured data from unstructured text input. For example:\n\n```python\nspec = \"Name: {name}\\nAge: {age:d}\\nActive: {active:bool}\"\ninput_str = \"Name: John\\nAge: 25\\nActive: yes\"\n\nscratchpad = Scratchpad(spec)\nresult = scratchpad.parse(input_str)\n\nprint(result)  # Output: {'name': 'John', 'age': 25, 'active': True}\n```\n\n`signals.py` handles signals related to user input and completion generation using Pydantic models as placeholders for specific actions. The `Generate` class is used to indicate that the completion generation process should be run, while the `GetInput` class is used to indicate that user input is required.\n\nIn summary, the `turbo-chat/structs` folder provides various components related to chat functionality, making it easier for other parts of the project to access and use these components.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/structs/summary.md"}}],["35",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/tree/master/.autodoc/docs/json/turbo_chat)\n\nThe `turbo-chat` project provides a framework for managing chat applications, including message handling, caching, memory management, and utility tools. The code is organized into several modules, each responsible for a specific functionality.\n\nThe main entry point for the project is the `__init__.py` file, which imports and exposes various components from different submodules, such as bots, cache, chat, config, errors, memory, structs, turbo, types, and utils. This simplifies the process of using these components in other parts of the project. For example, to use the `User` class, one would simply write:\n\n```python\nfrom turbo_chat import User\n```\n\nThe `chat.py` file provides a chat runner function, `run_chat`, which manages the conversation between a user and an AI assistant, utilizing memory and cache objects to optimize the response generation process.\n\nThe `config.py` file defines the available models for the project, which is likely a chatbot application utilizing OpenAI's GPT models. The purpose of this code is to provide a list of allowed models and a type definition for the models that can be used throughout the project.\n\nThe `errors.py` file defines custom error classes and a placeholder class for handling specific situations related to generators in the project.\n\nThe `runner.py` file is responsible for running a Turbo application using asynchronous generators. The main function in this code is `run`, which takes two arguments: `gen` and `input`. The `gen` argument is of type `TurboGenWrapper`, which is a custom type hint for the asynchronous generator. The `input` argument is optional and can be either a string or a dictionary.\n\nThe `turbo.py` file is a decorator for creating a chat application using an asynchronous generator. It provides a high-level interface for managing chat sessions, memory, caching, and debugging. The decorator can be applied to a generator function, which defines the chat application's behavior.\n\nThe project also includes several subfolders, such as `bots`, which provides various chatbot functionalities, such as question-answering, self-asking, subqueries handling, and text summarization; `cache`, which provides a simple in-memory caching mechanism; `memory`, which provides a memory management system with various storage strategies for handling chat data; `structs`, which contains code related to chat functionality, such as handling messages, processing results, and managing temporary data during chat sessions; `types`, which provides a framework for managing chat applications, including message handling, caching, memory management, and utility tools; and `utils`, which contains utility modules and functions that assist with various tasks related to language processing, retry mechanisms, template rendering, and token management in the context of a chat application.\n\nIn summary, the `turbo-chat` project offers a comprehensive framework for building and managing chat applications, with various components and utilities to handle different aspects of chat functionality, such as message handling, caching, memory management, and chatbot features.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/summary.md"}}],["36",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/turbo.py)\n\nThe `turbo` code is a decorator for creating a chat application using an asynchronous generator. It provides a high-level interface for managing chat sessions, memory, caching, and debugging. The decorator can be applied to a generator function, which defines the chat application's behavior.\n\nThe `turbo` decorator takes several optional parameters:\n\n- `memory_class`: A custom memory class (default is `LocalMemory`).\n- `model`: The OpenAI model to use (default is `\"gpt-3.5-turbo\"`).\n- `stream`: Whether to use streaming (not supported yet).\n- `cache_class`: A custom cache class.\n- `debug`: A callable for debugging purposes.\n- `**kwargs`: Additional keyword arguments for the OpenAI API.\n\nThe decorator wraps the generator function with a `turbo_gen_fn` function, which initializes the memory and cache classes, sets up the generator, and handles the different types of outputs yielded by the generator. The wrapped generator function can yield various types of objects, such as `Result`, `Message`, `BaseMessageCollection`, `GetInput`, and `Generate`. The `turbo_gen_fn` processes these objects accordingly, updating the memory, forwarding messages, and generating responses using the OpenAI API.\n\nThe `turbo_gen_fn` also supports debugging by passing a callable to the `debug` parameter. The debug function will receive input and output payloads with additional metadata, such as the generator function name and timestamp.\n\nHere's an example of how to use the `turbo` decorator:\n\n```python\nfrom turbo_chat import turbo, GetInput, Generate\n\n@turbo()\nasync def my_chat_app():\n    user_input = yield GetInput(\"What's your name?\")\n    yield Generate(f\"Hello, {user_input}. How can I help you today?\", forward=True)\n```\n\nIn this example, the `turbo` decorator is applied to the `my_chat_app` generator function. The generator yields a `GetInput` object to request the user's name and then yields a `Generate` object to create a response using the OpenAI API. The `forward=True` flag indicates that the generated response should be forwarded to the user.\n## Questions: \n 1. **Question**: What is the purpose of the `turbo` decorator and how does it work?\n   **Answer**: The `turbo` decorator is a parameterized decorator for creating a chatml app from an async generator. It takes various settings as input, such as memory_class, model, stream, cache_class, and debug. It wraps the given async generator function (gen_fn) with additional functionality, such as initializing memory and cache, handling different types of outputs, and managing the generator's lifecycle.\n\n2. **Question**: How does the `turbo` decorator handle different types of outputs from the wrapped generator?\n   **Answer**: The `turbo` decorator handles different types of outputs by checking the type of the output and performing specific actions accordingly. For example, if the output is a Result, it yields the output; if it's a Message, it appends it to memory and yields it if the forward flag is set; if it's a BaseMessageCollection, it extends the memory; if it's a GetInput, it appends an Assistant message to memory and waits for user input; and if it's a Generate, it runs the chat and yields the generated result if needed.\n\n3. **Question**: How does the `turbo` decorator handle debugging and logging?\n   **Answer**: The `turbo` decorator handles debugging and logging through the optional `debug` parameter, which is a callable function. If the `debug` function is provided, the decorator logs the inputs and outputs of the wrapped generator by calling the `debug` function with a dictionary containing the app name, timestamp, type (input or output), and payload (input or output data).","metadata":{"source":".autodoc/docs/markdown/turbo_chat/turbo.md"}}],["37",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/types/__init__.py)\n\nThe code provided is part of the `turbo-chat` project and serves as a central module that imports and exports various components related to caching, message handling, memory management, and utility tools. This module acts as a bridge between different functionalities, making it easier for other parts of the project to access and use these components.\n\nFirst, the code imports all the necessary components from their respective modules:\n\n- `cache`: Contains the `BaseCache` class, which is responsible for managing the caching mechanism in the project.\n- `generators`: Includes the `TurboGenWrapper` class, which is a wrapper for generator functions used in the project.\n- `memory`: Contains the `BaseMemory` class, which is responsible for managing the memory storage of the project.\n- `messages`: Includes the `MessageRole`, `Message`, `MessageDict`, and `BaseMessageCollection` classes, which are responsible for handling and managing messages within the project.\n- `tools`: Contains the `Tool` class, which is a utility class that provides various helper methods and tools for the project.\n\nAfter importing these components, the code defines the `__all__` variable, which is a list of strings representing the names of the components that should be exported when this module is imported by other parts of the project. This allows other modules to easily access and use these components by simply importing this central module.\n\nFor example, if another module in the project needs to use the `Message` class, it can import it as follows:\n\n```python\nfrom turbo_chat import Message\n\n# Now the Message class can be used in this module\n```\n\nBy providing a central module that imports and exports the necessary components, the code helps maintain a clean and organized project structure, making it easier for developers to understand and work with the project.\n## Questions: \n 1. **Question:** What is the purpose of the `# flake8: noqa` comment at the beginning of the code?\n   **Answer:** The `# flake8: noqa` comment is used to tell the Flake8 linter to ignore this file when checking for code style violations, such as the use of wildcard imports (`*`).\n\n2. **Question:** Why are wildcard imports (`*`) being used in this file, and what are the potential risks associated with using them?\n   **Answer:** Wildcard imports are used here to import all the names from the specified modules. However, using wildcard imports can lead to potential risks, such as name clashes and making it harder to understand which names are actually being imported and used in the code.\n\n3. **Question:** What is the purpose of the `__all__` list in this file?\n   **Answer:** The `__all__` list is used to define the public interface of this module, specifying which names should be imported when a client imports this module using a wildcard import. This helps to control the names that are exposed and prevent unintended names from being imported.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/types/__init__.md"}}],["38",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/types/cache.py)\n\nThe `turbo-chat` code provided defines an abstract base class `BaseCache` for caching agent responses. This class is designed to be extended by other cache implementations, providing a consistent interface for caching and retrieving data in the larger project.\n\n`BaseCache` inherits from two classes: `ABC`, which makes it an abstract base class, and `WithSetup`, which is a mixin that provides a setup method for initializing the cache. The class contains several methods for serializing, deserializing, and converting objects to cache keys, as well as abstract methods for cache operations.\n\nThe `serialize` method takes an object as input and returns it unchanged. This method can be overridden in derived classes to provide custom serialization logic. Similarly, the `deserialize` method takes a hashed object and returns it unchanged, allowing for custom deserialization logic in derived classes.\n\nThe `to_key` method takes an object, serializes it using the `serialize` method, and then converts it to a JSON string. This is used to create a cache key for storing and retrieving data.\n\nThe abstract methods `has`, `set`, `get`, and `clear` define the core cache operations:\n\n- `has`: Checks if a given key exists in the cache. Takes a key as input and returns a boolean.\n- `set`: Stores a value in the cache with a given key. Takes a key and a value as input and returns None.\n- `get`: Retrieves a value from the cache using a given key. Takes a key as input and returns the value.\n- `clear`: Clears the cache of all stored data. Returns None.\n\nThese methods are marked as `abstractmethod`, meaning they must be implemented by any derived class. They are also asynchronous, allowing for non-blocking cache operations in the larger project.\n\nExample usage in a derived class:\n\n```python\nclass CustomCache(BaseCache):\n    async def has(self, key: Any) -> bool:\n        # Custom implementation for checking if key exists\n        ...\n\n    async def set(self, key: Any, value: Any) -> None:\n        # Custom implementation for storing data in cache\n        ...\n\n    async def get(self, key: Any) -> Any:\n        # Custom implementation for retrieving data from cache\n        ...\n\n    async def clear(self) -> Any:\n        # Custom implementation for clearing cache\n        ...\n```\n\nBy providing a consistent interface for caching, `BaseCache` allows the `turbo-chat` project to easily switch between different cache implementations without modifying the core application logic.\n## Questions: \n 1. **Question:** What is the purpose of the `BaseCache` class and how is it intended to be used?\n\n   **Answer:** The `BaseCache` class is an abstract base class for caching agent responses. It provides a basic structure and common methods for implementing different caching strategies. Subclasses should implement the abstract methods to provide the actual caching functionality.\n\n2. **Question:** How does the `serialize` and `deserialize` methods work, and when are they used?\n\n   **Answer:** The `serialize` method is used to convert an object into a format that can be stored in the cache, while the `deserialize` method is used to convert the stored format back into the original object. By default, these methods do not perform any conversion and simply return the input object. They can be overridden in subclasses to provide custom serialization and deserialization logic.\n\n3. **Question:** What is the purpose of the `to_key` method and how does it work?\n\n   **Answer:** The `to_key` method is used to generate a unique string representation of an object, which can be used as a key in the cache. It works by first serializing the object using the `serialize` method, and then converting the serialized object into a JSON string. This ensures that the generated key is consistent and can be used to look up the cached value later.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/types/cache.md"}}],["39",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/types/generators.py)\n\nThis code is part of the `turbo-chat` project and defines the core components for generating messages using templates and managing the memory associated with the chat. The main purpose of this code is to provide a flexible and extensible way to generate messages in a chat application, allowing developers to easily customize the behavior of the chatbot.\n\nThe code starts by importing necessary modules and defining the `TurboGenWrapper` class, which is a wrapper around the main generator function. This class is responsible for managing the state of the generator and providing a convenient interface for interacting with it.\n\nThe `TurboGenTemplate` type is defined as an asynchronous generator that yields a union of `Message`, `Generate`, `GetInput`, and `Result` objects. This generator is responsible for producing messages based on the given template and context. The `TurboGenTemplateFn` protocol defines a callable object that takes an optional `BaseMemory` object and a context dictionary as input and returns a `TurboGenTemplate` generator.\n\nThe `TurboGenFn` protocol defines the main interface for interacting with the generator. It has two main methods: `configure` and `__call__`. The `configure` method takes a dictionary of new settings and returns a new instance of `TurboGenFn` with the updated settings. The `__call__` method takes a context dictionary as input and returns a `TurboGenWrapper` object that wraps the generator.\n\nHere's an example of how this code might be used in the larger project:\n\n```python\n# Define a custom TurboGenTemplateFn\nasync def custom_turbo_gen_template(memory: Optional[BaseMemory] = None, **context) -> TurboGenTemplate:\n    # Generate messages based on the given context and memory\n    ...\n\n# Create a TurboGenFn instance with the custom template function\nturbo_gen_fn = TurboGenFn(fn=custom_turbo_gen_template, settings={})\n\n# Configure the generator with new settings\nnew_settings = {\"setting1\": \"value1\", \"setting2\": \"value2\"}\nturbo_gen_fn = turbo_gen_fn.configure(new_settings)\n\n# Generate messages using the generator\nturbo_gen_wrapper = turbo_gen_fn(**context)\n```\n\nIn summary, this code provides a flexible and extensible way to generate messages in a chat application using templates and memory management. It allows developers to easily customize the behavior of the chatbot by defining their own generator functions and configuring the generator with custom settings.\n## Questions: \n 1. **Question:** What is the purpose of the `TurboGenTemplate` type alias?\n   **Answer:** The `TurboGenTemplate` type alias is used to define the expected return type of the `TurboGenTemplateFn` protocol. It is an asynchronous generator that yields a union of `Message`, `Generate`, `GetInput`, and `Result` types, and returns `Any`.\n\n2. **Question:** How does the `TurboGenTemplateFn` protocol work and what is its purpose?\n   **Answer:** The `TurboGenTemplateFn` protocol defines a callable interface that takes an optional `memory` parameter of type `BaseMemory` and any number of keyword arguments as context. It returns a `TurboGenTemplate` instance. This protocol is used to enforce a specific structure for functions that implement the TurboGen template functionality.\n\n3. **Question:** What is the role of the `TurboGenFn` protocol and how does it interact with the `TurboGenTemplateFn` protocol?\n   **Answer:** The `TurboGenFn` protocol defines an interface for a class that has a `TurboGenTemplateFn` function as its `fn` attribute and a dictionary as its `settings` attribute. It also has a `configure` method that takes a dictionary as input and returns a new instance of `TurboGenFn`. The `__call__` method of this protocol returns a `TurboGenWrapper` instance. This protocol is used to enforce a specific structure for classes that implement the TurboGen functionality and interact with the TurboGen template functions.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/types/generators.md"}}],["40",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/types/memory.py)\n\nThe `BaseMemory` class in this code serves as an abstract base class for persisting conversation history and state in the Turbo-Chat project. It inherits from `BaseMessageCollection`, `WithSetup`, and `pydantic.BaseModel` classes, which provide the necessary functionality for managing messages and setting up the memory.\n\nThe `BaseMemory` class has three abstract methods: `extend`, `get_state`, and `set_state`. These methods must be implemented by any concrete subclass of `BaseMemory`.\n\n- `extend`: This method takes a list of `Message` objects and is responsible for adding them to the conversation history. For example, a concrete implementation might store the messages in a database or an in-memory data structure.\n\n```python\nasync def extend(self, items: List[Message]) -> None:\n    ...\n```\n\n- `get_state`: This method returns the current state of the conversation as a dictionary. The state might include information such as the current topic, user preferences, or other relevant data.\n\n```python\nasync def get_state(self) -> dict:\n    ...\n```\n\n- `set_state`: This method takes a dictionary representing the new state of the conversation and an optional `merge` flag. If `merge` is set to `True`, the new state should be merged with the existing state; otherwise, the existing state should be replaced.\n\n```python\nasync def set_state(self, new_state: dict, merge: bool = False) -> None:\n    ...\n```\n\nAdditionally, the `BaseMemory` class provides two non-abstract methods: `append` and `prepare_prompt`.\n\n- `append`: This method takes a single `Message` object and adds it to the conversation history by calling the `extend` method with a list containing the message.\n\n```python\nasync def append(self, item: Message) -> None:\n    await self.extend([item])\n```\n\n- `prepare_prompt`: This method takes an optional `max_tokens` parameter and returns a list of `MessageDict` objects representing the conversation history. This list can be used as a prompt for an AI model, such as OpenAI's GPT-3. The method can be overridden in subclasses to add filtering or other transformations to the message history.\n\n```python\nasync def prepare_prompt(\n    self,\n    max_tokens: int = 0,\n) -> List[MessageDict]:\n    messages: List[MessageDict] = await self.get_dicts()\n    return messages\n```\n\nIn summary, the `BaseMemory` class provides a foundation for managing conversation history and state in the Turbo-Chat project. Concrete implementations of this class can store and manipulate conversation data in various ways, depending on the specific requirements of the project.\n## Questions: \n 1. **Question**: What is the purpose of the `BaseMemory` class and how does it relate to the overall functionality of the `turbo-chat` project?\n   **Answer**: The `BaseMemory` class serves as an abstract base class for persisting conversation history and state in the `turbo-chat` project. It provides a common interface for different memory implementations to store and manage chat messages and state.\n\n2. **Question**: How are the `extend`, `get_state`, and `set_state` methods expected to be implemented in subclasses of `BaseMemory`?\n   **Answer**: The `extend`, `get_state`, and `set_state` methods are marked as abstract methods, meaning that any subclass of `BaseMemory` must provide their own implementation for these methods to handle the storage and retrieval of chat messages and state.\n\n3. **Question**: What is the purpose of the `prepare_prompt` method and how does it interact with the message history?\n   **Answer**: The `prepare_prompt` method is responsible for turning the message history into a prompt for OpenAI. By default, it retrieves the message history as a list of dictionaries using the `get_dicts()` method, but it can be overridden in subclasses to add filtering or other transformations to the message history before it is used as a prompt.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/types/memory.md"}}],["41",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/types/messages.py)\n\nThe code in this file is responsible for handling messages in the turbo-chat project. It defines the structure of messages, their roles, and provides a base class for managing collections of messages.\n\nThe `MessageRole` is a type alias for a Literal, which defines the allowed values for message roles in the chat. These roles include \"system\", \"user\", \"assistant\", \"system name=example_user\", and \"system name=example_assistant\".\n\nThe `Message` class is a Pydantic model that represents a single chat message. It has the following attributes:\n\n- `role`: The role of the message sender, as defined by `MessageRole`.\n- `content`: The actual content of the message.\n- `template`: A template string for generating the content.\n- `variables`: A dictionary of variables to be used in the template.\n- `check`: A boolean flag to indicate whether to check template variables.\n- `forward`: A boolean flag to indicate whether the message should be forwarded downstream.\n\nThe `validate_content_template` method is a Pydantic root validator that ensures either `content` or `template`/`variables` are set, but not both. If a template is provided, it renders the template using the provided variables and sets the `content` attribute.\n\nThe `MessageDict` is a TypedDict that defines the structure of a message dictionary with two keys: `role` and `content`.\n\nThe `BaseMessageCollection` is an abstract base class for managing collections of messages. It has two abstract methods:\n\n- `get`: An asynchronous method that should be implemented by subclasses to return a list of `Message` objects.\n- `get_dicts`: An asynchronous method that returns a list of `MessageDict` objects. It calls the `get` method to retrieve the messages and then converts them to dictionaries using the `dict` method of the `Message` class.\n\nIn the larger project, this code would be used to manage and manipulate messages in the chat system. For example, when a new message is received, it could be added to a collection of messages, and the collection could be used to render the chat history or perform other operations on the messages.\n## Questions: \n 1. **Question**: What is the purpose of the `MessageRole` type and what are the allowed values for it?\n   **Answer**: `MessageRole` is an enumeration type that represents the allowed values for the role of a message in the chat. The allowed values are: \"system\", \"user\", \"assistant\", \"system name=example_user\", and \"system name=example_assistant\".\n\n2. **Question**: How does the `validate_content_template` method work in the `Message` class?\n   **Answer**: The `validate_content_template` method is a Pydantic root validator that checks if either the `content` or the `template` and `variables` are set for a message. If the `template` and `variables` are set, it renders the template using the provided variables and assigns the result to the `content` field.\n\n3. **Question**: What is the purpose of the `BaseMessageCollection` abstract class and its methods?\n   **Answer**: The `BaseMessageCollection` abstract class serves as a base class for asynchronous collections of prefix messages. It has two methods: `get`, which is an abstract method that should be implemented by subclasses to return a list of `Message` objects, and `get_dicts`, which is an asynchronous method that returns a list of `MessageDict` objects, which are dictionaries containing only the \"role\" and \"content\" fields of the messages.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/types/messages.md"}}],["42",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/types/misc.py)\n\nThe `WithSetup` class in the given code snippet is designed to provide a base class for other classes in the Turbo-Chat project that require an asynchronous setup process. This class defines an asynchronous method called `setup`, which takes any number of keyword arguments and returns `None`. The purpose of this method is to perform any necessary setup tasks before the main functionality of the derived class is executed.\n\nThe `setup` method is defined as an asynchronous method using the `async def` syntax. This means that it is designed to be used with Python's `asyncio` library, which allows for asynchronous programming using coroutines. Asynchronous programming is particularly useful in applications like chat systems, where multiple tasks need to be performed concurrently, such as sending and receiving messages, updating the user interface, and handling user input.\n\nIn the Turbo-Chat project, the `WithSetup` class can be used as a base class for any component that requires an asynchronous setup process. To use this class, a developer would create a new class that inherits from `WithSetup` and then override the `setup` method to implement the specific setup tasks required for that component. For example:\n\n```python\nclass ChatClient(WithSetup):\n    async def setup(self, username: str, server_address: str) -> None:\n        self.username = username\n        self.server_address = server_address\n        # Connect to the chat server and perform other setup tasks\n```\n\nIn this example, a `ChatClient` class is created that inherits from `WithSetup`. The `setup` method is overridden to take two keyword arguments, `username` and `server_address`, and perform the necessary setup tasks for the chat client, such as connecting to the chat server.\n\nTo use the `ChatClient` class, a developer would create an instance of the class and then call the `setup` method using the `await` keyword, which is used to call asynchronous methods in Python:\n\n```python\nasync def main():\n    client = ChatClient()\n    await client.setup(username=\"JohnDoe\", server_address=\"chat.example.com\")\n```\n\nBy using the `WithSetup` class as a base class, the Turbo-Chat project can ensure that all components with asynchronous setup processes follow a consistent pattern, making the code easier to understand and maintain.\n## Questions: \n 1. **Question:** What is the purpose of the `WithSetup` class and its `setup` method in the context of the turbo-chat project?\n   **Answer:** The `WithSetup` class and its `setup` method might be used for setting up necessary configurations or initializations for the turbo-chat project, but more context is needed to determine its exact purpose.\n\n2. **Question:** Are there any specific keyword arguments that should be passed to the `setup` method, or is it designed to handle any arbitrary set of keyword arguments?\n   **Answer:** The `setup` method accepts arbitrary keyword arguments, but without more context or documentation, it's unclear which specific arguments are expected or how they are used within the method.\n\n3. **Question:** Are there any other methods or attributes in the `WithSetup` class that interact with the `setup` method, or is it meant to be used as a standalone method?\n   **Answer:** Based on the provided code snippet, it's unclear if there are any other methods or attributes in the `WithSetup` class that interact with the `setup` method. More information about the class implementation is needed to determine its usage.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/types/misc.md"}}],["43",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/tree/master/.autodoc/docs/json/turbo_chat/types)\n\nThe `turbo-chat` project provides a framework for managing chat applications, including message handling, caching, memory management, and utility tools. The code is organized into several modules, each responsible for a specific functionality.\n\nThe `cache.py` module defines the `BaseCache` class, an abstract base class for caching agent responses. It provides a consistent interface for caching and retrieving data, allowing the project to easily switch between different cache implementations.\n\nThe `generators.py` module defines the `TurboGenWrapper` class and related types for generating messages using templates and managing chat memory. This allows developers to customize the chatbot's behavior by defining their own generator functions and configuring the generator with custom settings.\n\nThe `memory.py` module provides the `BaseMemory` class, an abstract base class for persisting conversation history and state. Concrete implementations of this class can store and manipulate conversation data in various ways, depending on the project's requirements.\n\nThe `messages.py` module handles messages in the chat system, defining the structure of messages, their roles, and providing a base class for managing collections of messages. This module is used to manage and manipulate messages in the chat system, such as rendering chat history or performing other operations on the messages.\n\nThe `misc.py` module defines the `WithSetup` class, a base class for components that require an asynchronous setup process. By using this class as a base class, the project can ensure that all components with asynchronous setup processes follow a consistent pattern, making the code easier to understand and maintain.\n\nThe `tools.py` module defines a type alias called `Tool` that represents a specific type of callable function. This type alias is used throughout the project to represent functions that perform certain operations on strings and return the result asynchronously.\n\nHere's an example of how these components might be used together in the larger project:\n\n```python\nfrom turbo_chat import CustomCache, TurboGenFn, ChatClient, process_messages\n\n# Create a custom cache instance\ncache = CustomCache()\n\n# Define a custom TurboGenTemplateFn and create a TurboGenFn instance\nturbo_gen_fn = TurboGenFn(fn=custom_turbo_gen_template, settings={})\n\n# Configure the generator with new settings\nnew_settings = {\"setting1\": \"value1\", \"setting2\": \"value2\"}\nturbo_gen_fn = turbo_gen_fn.configure(new_settings)\n\n# Create a ChatClient instance and set up the connection\nclient = ChatClient()\nawait client.setup(username=\"JohnDoe\", server_address=\"chat.example.com\")\n\n# Process messages using a Tool function\nmessages = [\"Hello\", \"How are you?\"]\nprocessed_messages = await process_messages(messages, tool=my_tool_function)\n```\n\nIn this example, a custom cache instance is created, a custom generator function is defined and configured, a chat client is set up, and messages are processed using a `Tool` function. This demonstrates how the various components of the `turbo-chat` project can be used together to build a complete chat application.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/types/summary.md"}}],["44",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/types/tools.py)\n\nThe code provided defines a type alias called `Tool` that represents a specific type of callable function. This type alias is likely used throughout the turbo-chat project to represent functions that perform certain operations on strings and return the result asynchronously.\n\nThe `Tool` type alias is defined using the `Callable` and `Awaitable` types from the `typing` module. The `Callable` type is used to represent a function that can be called with a specific set of arguments and return a specific type. In this case, the `Tool` type alias represents a function that takes a single argument of type `str` (a string) and returns an `Awaitable[str]`.\n\nThe `Awaitable` type is used to represent an object that can be used with the `await` keyword in an asynchronous context. In this case, the `Awaitable[str]` type means that the function represented by the `Tool` type alias is expected to return a string asynchronously, i.e., the function is likely defined using the `async def` syntax.\n\nThe `__all__` variable is a list that defines the public interface of this module. It specifies which names should be imported when a client imports this module using a wildcard import (e.g., `from module_name import *`). In this case, the `__all__` list contains only the `Tool` type alias, indicating that this is the only name that should be imported from this module.\n\nHere's an example of how the `Tool` type alias might be used in the turbo-chat project:\n\n```python\nfrom typing import List\nfrom .tool_type import Tool\n\nasync def process_messages(messages: List[str], tool: Tool) -> List[str]:\n    processed_messages = []\n    for message in messages:\n        processed_message = await tool(message)\n        processed_messages.append(processed_message)\n    return processed_messages\n```\n\nIn this example, the `process_messages` function takes a list of messages and a `Tool` function as arguments. It processes each message using the provided `Tool` function and returns a list of processed messages.\n## Questions: \n 1. **What is the purpose of the `Tool` type alias in this code?**\n\n   The `Tool` type alias is defined as a `Callable` that takes a single `str` argument and returns an `Awaitable[str]`. This is used to represent a function or a method that takes a string input and returns an awaitable string object, which can be used in asynchronous programming.\n\n2. **What is the significance of the `__all__` variable in this code?**\n\n   The `__all__` variable is a list that defines the public interface of this module. It specifies which names should be imported when a client imports this module using a wildcard import (e.g., `from turbo_chat import *`). In this case, only the `Tool` type alias is part of the public interface.\n\n3. **Why are the `Awaitable` and `Callable` types imported from the `typing` module?**\n\n   The `Awaitable` and `Callable` types are imported from the `typing` module to provide type hints for the `Tool` type alias. These type hints help developers understand the expected input and output types for the `Tool` and can also be used by static type checkers to catch potential type-related issues in the code.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/types/tools.md"}}],["45",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/utils/__init__.py)\n\nThe code provided is part of a larger project and serves as a utility module that imports and exposes various functionalities related to language processing, retry mechanisms, template rendering, and token management in the context of a chat application.\n\nThe code starts by importing all functions and classes from four different modules:\n\n1. `lang`: This module likely contains functions and classes related to language processing and manipulation, such as inflection or other natural language processing tasks.\n2. `retries`: This module provides functions and classes for implementing retry mechanisms, which can be useful when dealing with network requests or other operations that may fail and need to be retried.\n3. `template`: This module contains functions and classes for rendering templates, which can be used to generate dynamic content based on user input or other data.\n4. `tokens`: This module deals with token management, such as counting tokens and determining the maximum token length.\n\nAfter importing the necessary functions and classes, the code defines a list called `__all__` that explicitly specifies the functions and classes that should be exposed when this module is imported by other parts of the project. The following functions are included in the `__all__` list:\n\n- `inflect`: A function from the `lang` module that likely performs inflection or other language processing tasks.\n- `render_template`: A function from the `template` module that renders templates based on input data.\n- `create_retry_decorator`: A function from the `retries` module that creates a retry decorator, which can be used to wrap functions that need retry mechanisms.\n- `with_retries`: A function from the `retries` module that can be used as a context manager to execute a block of code with retries.\n- `count_tokens`: A function from the `tokens` module that counts the number of tokens in a given input.\n- `get_max_tokens_length`: A function from the `tokens` module that returns the maximum token length for a given input.\n\nBy exposing these functions, the module allows other parts of the project to easily access and use these utilities for various tasks related to language processing, retry mechanisms, template rendering, and token management.\n## Questions: \n 1. **What is the purpose of `flake8: noqa` at the beginning of the code?**\n\n   The `flake8: noqa` comment is used to tell the flake8 linter to ignore this file for linting, which means it won't raise any warnings or errors for this file.\n\n2. **Why are there wildcard imports (`*`) being used in this file?**\n\n   Wildcard imports are used here to import all the functions and classes from the specified modules, making them available for use in the `turbo-chat` project. However, it's worth noting that using wildcard imports is generally discouraged, as it can lead to confusion and potential naming conflicts.\n\n3. **What is the purpose of the `__all__` list in this code?**\n\n   The `__all__` list is used to define the public interface of this module. It specifies which functions and classes should be imported when a client imports the module using a wildcard import (e.g., `from turbo_chat import *`). This helps to keep the module's namespace clean and makes it clear which functions and classes are part of the public API.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/utils/__init__.md"}}],["46",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/utils/args.py)\n\nThe code in this file provides utility functions to work with function arguments in the larger turbo-chat project. It helps to ensure that the required arguments for a specific function are provided when calling that function.\n\nThe first function, `get_required_args(fn: Callable) -> Set[str]`, takes a callable (function) as its input and returns a set of required argument names for that function. It does this by using the `inspect` module to get the signature of the function and then filtering the parameters based on their kind (positional only, positional or keyword, and keyword only). The function returns a set of required parameter names.\n\n```python\ndef example_function(a, b, c=3, *, d, e=5):\n    pass\n\nrequired_args = get_required_args(example_function)\n# required_args will be {'a', 'b', 'd'}\n```\n\nThe second function, `ensure_args(fn: Callable, args: dict) -> bool`, takes a callable (function) and a dictionary of arguments as its inputs. It checks if the provided dictionary of arguments contains all the required arguments for the given function. The function returns a boolean value indicating whether all required arguments are present in the dictionary.\n\n```python\ndef example_function(a, b, c=3, *, d, e=5):\n    pass\n\nargs = {'a': 1, 'b': 2, 'd': 4}\nresult = ensure_args(example_function, args)\n# result will be True\n\nargs = {'a': 1, 'b': 2}\nresult = ensure_args(example_function, args)\n# result will be False\n```\n\nThese utility functions can be used in the turbo-chat project to validate the presence of required arguments when calling functions, ensuring that the functions are called with the correct set of arguments and preventing potential errors due to missing arguments.\n## Questions: \n 1. **Question:** What is the purpose of the `get_required_args` function, and how does it determine which arguments are required?\n\n   **Answer:** The `get_required_args` function is used to get the required arguments for a given function `fn`. It does this by inspecting the function's signature, filtering the parameters based on their kind (positional only, positional or keyword, and keyword only), and checking if they have a default value or not.\n\n2. **Question:** How does the `ensure_args` function work, and what does it return?\n\n   **Answer:** The `ensure_args` function checks if the given `args` dictionary contains all the required arguments for the function `fn`. It does this by getting the required arguments using `get_required_args` function, and then checking if the intersection of the required arguments and the keys of the `args` dictionary is equal to the set of required arguments. The function returns `True` if all required arguments are present in the `args` dictionary, and `False` otherwise.\n\n3. **Question:** Can the `ensure_args` function handle cases where the function `fn` has default values for some of its required arguments?\n\n   **Answer:** Yes, the `ensure_args` function can handle cases where the function `fn` has default values for some of its required arguments. The `get_required_args` function, which is used by `ensure_args`, filters out parameters with default values, so only the truly required arguments without default values are considered when checking if the `args` dictionary has all the required arguments.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/utils/args.md"}}],["47",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/utils/fn.py)\n\nThe `turbo-chat` project contains a utility function called `pick` that is used to extract specific keys from a given dictionary and return a new dictionary containing only those keys. This function can be helpful in scenarios where you need to filter out certain keys from a dictionary, for example, when processing user input or working with API responses.\n\nThe `pick` function takes three arguments:\n\n1. `dictionary`: The input dictionary from which keys need to be extracted.\n2. `keys`: A list of strings representing the keys to be picked from the input dictionary.\n3. `optional`: An optional argument that, if provided, will be used as the default value for keys not found in the input dictionary. If not provided, a special class `PickOptionalOff` is used as the default value.\n\nThe function returns a new dictionary containing only the specified keys and their corresponding values from the input dictionary. If a key is not found in the input dictionary and the `optional` argument is not provided, the key will not be included in the output dictionary. If the `optional` argument is provided, the key will be included in the output dictionary with the value set to the `optional` argument.\n\nHere's an example of how the `pick` function can be used:\n\n```python\ninput_dict = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\nkeys_to_pick = [\"name\", \"city\"]\n\noutput_dict = pick(input_dict, keys_to_pick)\n# output_dict will be {\"name\": \"John\", \"city\": \"New York\"}\n\nkeys_to_pick_with_optional = [\"name\", \"country\"]\noutput_dict_with_optional = pick(input_dict, keys_to_pick_with_optional, \"Unknown\")\n# output_dict_with_optional will be {\"name\": \"John\", \"country\": \"Unknown\"}\n```\n\nIn the larger project, the `pick` function can be used to filter out specific keys from dictionaries, making it easier to work with data structures and ensuring that only relevant information is processed or passed on to other parts of the application.\n## Questions: \n 1. **Question:** What is the purpose of the `PickOptionalOff` class?\n   **Answer:** The `PickOptionalOff` class is used as a default value for the `optional` parameter in the `pick` function. It helps to differentiate between cases when the `optional` parameter is not provided and when it is provided with a value of `None`.\n\n2. **Question:** How does the `pick` function handle missing keys in the input dictionary?\n   **Answer:** If a key is missing in the input dictionary and the `optional` parameter is not provided or is an instance of `PickOptionalOff`, the function will use the `dictionary.get(key, optional)` method, which returns the value of the key if it exists, or the value of `optional` (which is an instance of `PickOptionalOff`) if the key is missing. If the `optional` parameter is provided with a value other than an instance of `PickOptionalOff`, the function will raise a KeyError if the key is missing.\n\n3. **Question:** What is the expected output format of the `pick` function?\n   **Answer:** The `pick` function returns a new dictionary containing the specified keys from the input dictionary and their corresponding values. If a key is missing in the input dictionary and the `optional` parameter is not provided or is an instance of `PickOptionalOff`, the output dictionary will have the key with a value of an instance of `PickOptionalOff`. If the `optional` parameter is provided with a value other than an instance of `PickOptionalOff`, the output dictionary will have the key with the provided `optional` value.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/utils/fn.md"}}],["48",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/utils/lang.py)\n\nThe `turbo-chat` project contains a module that focuses on inflecting words based on their grammatical tags. This module is designed to be a utility for the larger project, which may involve processing and manipulating natural language text.\n\nThe module imports the `getInflection` function from the `lemminflect` library, which is a popular library for lemmatization and inflection of English words. The main purpose of this module is to provide a simple interface for inflecting words using the `inflect` function.\n\nThe `inflect` function takes two arguments: `word` and `tag`. The `word` argument is the input word that needs to be inflected, and the `tag` argument is the grammatical tag that the word should be inflected to. The function then calls the `getInflection` function from the `lemminflect` library with the given `word` and `tag` arguments. The `getInflection` function returns a tuple containing the inflected word and its grammatical tag. The `inflect` function returns only the inflected word (the first element of the tuple) to the caller.\n\nHere's an example of how the `inflect` function can be used:\n\n```python\ninflected_word = inflect(\"run\", \"VBD\")\nprint(inflected_word)  # Output: \"ran\"\n```\n\nIn this example, the input word \"run\" is inflected to its past tense form \"ran\" based on the grammatical tag \"VBD\" (verb, past tense). This module can be used in the larger `turbo-chat` project for tasks that involve generating or manipulating text, such as generating responses in a chatbot or analyzing user input. By providing a simple interface for inflecting words, this module makes it easier for other parts of the project to work with natural language text.\n## Questions: \n 1. **Question:** What is the purpose of the `__all__` variable in this code?\n   **Answer:** The `__all__` variable is used to define the public interface of the module, specifying which names should be imported when a client imports the module using a wildcard import (e.g., `from turbo_chat import *`).\n\n2. **Question:** What is the `lemminflect` library and how is it being used in this code?\n   **Answer:** The `lemminflect` library is a Python library for inflecting words and lemmatizing them. In this code, it is being used to inflect a given word based on the provided part-of-speech tag using the `getInflection` function.\n\n3. **Question:** What are the expected input and output types for the `inflect` function?\n   **Answer:** The `inflect` function expects a string `word` and a string `tag` as input, and it returns a string representing the inflected form of the input word based on the provided part-of-speech tag.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/utils/lang.md"}}],["49",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/utils/retries.py)\n\nThis code defines a retry mechanism for handling API calls in the `turbo-chat` project. The main purpose of this code is to provide a way to automatically retry API calls when certain exceptions occur, such as timeouts, API errors, connection errors, rate limit errors, and service unavailability errors. This can help improve the reliability and robustness of the project when interacting with external APIs.\n\nThe code imports necessary modules and defines two main components: `create_retry_decorator` function and `with_retries` decorator.\n\nThe `create_retry_decorator` function takes three optional arguments: `min_seconds`, `max_seconds`, and `max_retries`. It returns a retry decorator that can be applied to any function or method. The decorator will retry the function or method when any of the specified exceptions occur, with an exponential backoff strategy. The backoff starts with a minimum wait time of `min_seconds` (default is 4 seconds) and increases exponentially up to a maximum of `max_seconds` (default is 10 seconds). The maximum number of retries is specified by `max_retries` (default is 5).\n\nHere's an example of how the `create_retry_decorator` function can be used:\n\n```python\ncustom_retry_decorator = create_retry_decorator(min_seconds=2, max_seconds=8, max_retries=3)\n\n@custom_retry_decorator\ndef make_api_call():\n    # Code to make an API call\n    pass\n```\n\nThe `with_retries` decorator is a default retry decorator created using the `create_retry_decorator` function with default arguments. It can be applied to any function or method to automatically retry the function or method when any of the specified exceptions occur, with the default exponential backoff strategy.\n\nHere's an example of how the `with_retries` decorator can be used:\n\n```python\n@with_retries\ndef make_api_call():\n    # Code to make an API call\n    pass\n```\n\nIn the larger project, this retry mechanism can be applied to any function or method that makes API calls, ensuring that temporary issues like timeouts or rate limits are handled gracefully and do not cause the entire process to fail.\n## Questions: \n 1. **Question:** What is the purpose of the `create_retry_decorator` function and how does it work?\n\n   **Answer:** The `create_retry_decorator` function is used to create a retry decorator that can be applied to other functions. It takes in parameters for minimum and maximum wait times between retries, and the maximum number of retries. It uses the `tenacity` library to create a retry decorator that will retry the function if any of the specified exceptions from the `openai` library are raised.\n\n2. **Question:** What is the purpose of the `with_retries` variable?\n\n   **Answer:** The `with_retries` variable is a default retry decorator created using the `create_retry_decorator` function with its default parameters. It can be used to easily apply the retry functionality to other functions without having to create a new retry decorator each time.\n\n3. **Question:** How can the `with_retries` decorator be used with other functions in the code?\n\n   **Answer:** To use the `with_retries` decorator with other functions, simply add the `@with_retries` decorator above the function definition. This will apply the retry functionality to the function, causing it to retry if any of the specified exceptions from the `openai` library are raised, according to the parameters set in the `create_retry_decorator` function.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/utils/retries.md"}}],["50",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/tree/master/.autodoc/docs/json/turbo_chat/utils)\n\nThe `turbo_chat/utils` folder contains utility modules and functions that assist with various tasks related to language processing, retry mechanisms, template rendering, and token management in the context of a chat application.\n\nFor example, the `lang.py` module provides an `inflect` function that inflects words based on their grammatical tags. This can be useful when generating or manipulating text, such as generating responses in a chatbot or analyzing user input:\n\n```python\ninflected_word = inflect(\"run\", \"VBD\")\nprint(inflected_word)  # Output: \"ran\"\n```\n\nThe `retries.py` module defines a retry mechanism for handling API calls. It provides a `create_retry_decorator` function and a `with_retries` decorator that can be applied to any function or method that makes API calls, ensuring that temporary issues like timeouts or rate limits are handled gracefully:\n\n```python\n@with_retries\ndef make_api_call():\n    # Code to make an API call\n    pass\n```\n\nThe `template.py` module is responsible for rendering templates using the Jinja2 templating engine. It provides a `render_template` function that renders templates with optional validation of the input variables:\n\n```python\ntemplate_string = \"Hello, {{ name|inflect('capitalize') }}!\"\nvariables = {\"name\": \"john\"}\nrendered_string = render_template(template_string, variables, check=True)\nprint(rendered_string)  # Output: \"Hello, John!\"\n```\n\nThe `tokens.py` module provides utility functions to work with OpenAI models, focusing on token counting and handling model-specific token limits. It implements two functions: `get_max_tokens_length` and `count_tokens`:\n\n```python\nmessages = [{\"content\": \"Hello, how are you?\"}, {\"content\": \"I'm fine, thank you!\"}]\nmodel = TurboModel(\"gpt-3.5-turbo\")\ntotal_tokens = count_tokens(messages, model)\n```\n\nThe `args.py` module provides utility functions to work with function arguments, ensuring that the required arguments for a specific function are provided when calling that function:\n\n```python\ndef example_function(a, b, c=3, *, d, e=5):\n    pass\n\nargs = {'a': 1, 'b': 2, 'd': 4}\nresult = ensure_args(example_function, args)\n# result will be True\n```\n\nThe `fn.py` module contains a utility function called `pick` that extracts specific keys from a given dictionary and returns a new dictionary containing only those keys:\n\n```python\ninput_dict = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}\nkeys_to_pick = [\"name\", \"city\"]\n\noutput_dict = pick(input_dict, keys_to_pick)\n# output_dict will be {\"name\": \"John\", \"city\": \"New York\"}\n```\n\nThese utility modules and functions can be used throughout the `turbo-chat` project to simplify various tasks and improve the overall code quality and maintainability.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/utils/summary.md"}}],["51",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/utils/template.py)\n\nThe code in this file is responsible for rendering templates using the Jinja2 templating engine. It sets up a Jinja2 environment, adds custom filters, and provides a function to render templates with optional validation of the input variables.\n\nThe Jinja2 environment is configured with `autoescape=False`, `trim_blocks=True`, and `lstrip_blocks=True`. This means that the environment will not automatically escape HTML characters, will remove the first newline after a block, and will strip leading whitespace from block tags.\n\nA custom filter called `inflect` is added to the Jinja2 environment. This filter is imported from the `.lang` module and can be used in templates to apply inflections to words.\n\nThe main function provided by this file is `render_template`, which takes a template string, a dictionary of variables, and an optional `check` flag. The function first parses the template string using the Jinja2 environment. If the `check` flag is set to `True`, the function will infer the required variables from the template and validate the input variables against the inferred JSON schema using the `jsonschema` library. Finally, the function renders the template with the provided variables and returns the rendered string.\n\nHere's an example of how this function might be used in the larger project:\n\n```python\ntemplate_string = \"Hello, {{ name|inflect('capitalize') }}!\"\nvariables = {\"name\": \"john\"}\nrendered_string = render_template(template_string, variables, check=True)\nprint(rendered_string)  # Output: \"Hello, John!\"\n```\n\nIn this example, the `render_template` function is used to render a simple greeting template with a name variable. The `inflect` filter is used to capitalize the name. The `check` flag is set to `True`, so the function will validate the input variables before rendering the template.\n## Questions: \n 1. **Question:** What is the purpose of the `flake8: noqa` comment at the beginning of the code?\n   **Answer:** The `flake8: noqa` comment is used to tell the Flake8 linter to ignore this file when checking for code style violations. This is done because the file contains wildcard imports, which are generally discouraged but are allowed in this specific case.\n\n2. **Question:** How does the `render_template` function work and what is the purpose of the `check` parameter?\n   **Answer:** The `render_template` function takes a Jinja2 template string and a dictionary of variables, and returns the rendered template as a string. The `check` parameter, when set to `True`, enables validation of the provided variables against the inferred JSON schema of the template to ensure that the required variables are present and have the correct types.\n\n3. **Question:** What is the purpose of the `inflect` filter added to the `jinja_env`?\n   **Answer:** The `inflect` filter is a custom filter added to the Jinja2 environment, which allows for the transformation of words in the template based on grammatical rules (e.g., pluralization, conjugation). This filter can be used within the Jinja2 templates to apply these transformations on the fly.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/utils/template.md"}}],["52",{"pageContent":"[View code on GitHub](https://github.com/creatorrr/turbo-chat/blob/master/turbo_chat/utils/tokens.py)\n\nThis code provides utility functions to work with OpenAI models in the `turbo-chat` project, specifically focusing on token counting and handling model-specific token limits. It imports necessary libraries, defines a dictionary of model token windows, and implements two functions: `get_max_tokens_length` and `count_tokens`.\n\nThe `MODEL_WINDOWS` dictionary maps OpenAI model names to their respective token limits. For example, \"gpt-4\" has a limit of 8,192 tokens, while \"gpt-3.5-turbo\" has a limit of 4,096 tokens.\n\nThe `get_max_tokens_length` function takes a model name as input and returns the maximum token length for that model. It iterates through the `MODEL_WINDOWS` dictionary and checks if the input model name starts with the model prefix. If it finds a match, it returns the corresponding token limit. If the model name is not found, it raises a `ValueError`.\n\n```python\nmax_tokens = get_max_tokens_length(\"gpt-3.5-turbo\")\n```\n\nThe `count_tokens` function takes a list of messages and a `TurboModel` object as input and returns the total number of tokens in the messages. It first retrieves the appropriate encoding for the model using `tiktoken.encoding_for_model`. Then, it checks if the model is \"gpt-3.5-turbo-0301\" and calculates the token count accordingly. For other models, it simply sums up the tokens in the messages' content.\n\n```python\nmessages = [{\"content\": \"Hello, how are you?\"}, {\"content\": \"I'm fine, thank you!\"}]\nmodel = TurboModel(\"gpt-3.5-turbo\")\ntotal_tokens = count_tokens(messages, model)\n```\n\nThese utility functions can be used in the larger `turbo-chat` project to ensure that the input and output tokens stay within the model's token limits, preventing errors and optimizing the usage of OpenAI API calls.\n## Questions: \n 1. **Question**: What is the purpose of the `get_max_tokens_length` function and how does it determine the maximum token length for a given model?\n   **Answer**: The `get_max_tokens_length` function returns the maximum token length for a given model by checking if the model name starts with any of the keys in the `MODEL_WINDOWS` dictionary. If a match is found, it returns the corresponding value as the maximum token length.\n\n2. **Question**: How does the `count_tokens` function handle token counting differently for the \"gpt-3.5-turbo-0301\" model compared to other models?\n   **Answer**: For the \"gpt-3.5-turbo-0301\" model, the `count_tokens` function calculates the number of tokens by iterating through each message and considering the role/name and content tokens, as well as additional tokens for message formatting. For other models, it simply sums up the tokens for the content of each message, without considering role/name or formatting tokens.\n\n3. **Question**: What is the purpose of the `tiktoken` library in this code and how is it used in the `count_tokens` function?\n   **Answer**: The `tiktoken` library is used to encode text into tokens for a specific model. In the `count_tokens` function, it is used to obtain the encoding for the given `TurboModel` and then encode the content of each message into tokens, which are then counted to determine the total number of tokens.","metadata":{"source":".autodoc/docs/markdown/turbo_chat/utils/tokens.md"}}]]